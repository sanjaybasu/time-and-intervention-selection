{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826a077b-867f-479c-8e1e-76c931db034e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "BLOCK 1 â€” FULL DATA BUILD (Standalone)\n",
    "======================================\n",
    "End-to-end feature build from databases. Produces everything later blocks need\n",
    "WITHOUT depending on prior outputs.\n",
    "\n",
    "What this builds\n",
    "----------------\n",
    "â€¢ Members (patient_id) with demographics\n",
    "â€¢ ADT events (ed_visit/admission) with Memberâ†’patient_id mapping\n",
    "â€¢ Interventions from EncounterNote *that actually occurred* (encounterOccurred='YES'),\n",
    "  restricted to *engaged* patients by Patient.status âˆˆ ENGAGED_STATUSES\n",
    "â€¢ Diagnoses free-text â†’ clinical categories â†’ per-patient dx_cat_* features\n",
    "â€¢ Leakâ€‘free features and binary outcomes for 7/30/90/180 days from a cutoff\n",
    "â€¢ Helper artifacts for downstream blocks\n",
    "\n",
    "Outputs (written to ./publication_outputs/ unless noted)\n",
    "--------------------------------------------------------\n",
    "â€¢ feature_matrix.parquet  (and mirror features_df.parquet)\n",
    "â€¢ feature_meta.json  (+ artifacts/feature_cols.json)\n",
    "â€¢ interventions_df.csv   (occurred YES, engaged only)\n",
    "â€¢ adt_events_clean.csv   (debug/QA optional)\n",
    "â€¢ signal_risks.csv       (if available)\n",
    "â€¢ run_meta.json          (provenance)\n",
    "\n",
    "Notes\n",
    "-----\n",
    "â€¢ Preserves your complex ID mapping: Core Member.id â†’ Member.external_identifier (patient_id)\n",
    "â€¢ Uses cutoff_date of 2024-12-31 (training period end date)\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "import os, sys, json, re\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Waymark DB setup\n",
    "# ---------------------------------------------------------------------------\n",
    "sagemaker_lib = os.path.expanduser(\"~/sagemaker-lib\")\n",
    "if sagemaker_lib not in sys.path:\n",
    "    sys.path.insert(0, sagemaker_lib)\n",
    "import waymark  # provided by your environment\n",
    "\n",
    "CORE_ENGINE = waymark.get_waymark_core_db_engine()\n",
    "LH_ENGINE   = waymark.get_lighthouse_db_engine()\n",
    "\n",
    "OUTPUT_DIR = Path(\"publication_outputs\"); OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "ART_DIR    = Path(\"artifacts\"); ART_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Engaged statuses for Patient.status\n",
    "ENGAGED_STATUSES = [\n",
    "    \"ACTIVATED\", \"IN_CONTACT\", \"GRADUATED\", \"ONBOARDED\",\n",
    "    \"MODERATE\", \"PREGRADUATION\", \"MAINTENANCE\", \"HIGH\"\n",
    "]\n",
    "\n",
    "# Cutoff for temporal split (no leakage)\n",
    "CUTOFF_DATE = pd.Timestamp(\"2024-12-31\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Utilities\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def _read_sql(sql: str, engine) -> pd.DataFrame:\n",
    "    with engine.connect() as conn:\n",
    "        return pd.read_sql(sql, conn)\n",
    "\n",
    "\n",
    "def _member_map() -> pd.DataFrame:\n",
    "    sql = '''\n",
    "        SELECT \"id\" AS member_id, \"external_identifier\" AS patient_id\n",
    "        FROM \"Member\"\n",
    "        WHERE \"external_identifier\" IS NOT NULL AND \"external_identifier\" <> ''\n",
    "    '''\n",
    "    df = _read_sql(sql, CORE_ENGINE)\n",
    "    df[\"member_id\"] = df[\"member_id\"].astype(str)\n",
    "    df[\"patient_id\"] = df[\"patient_id\"].astype(str)\n",
    "    return df\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 1) Members (demographics)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def extract_members() -> pd.DataFrame:\n",
    "    print(\"\\nðŸ“Š Extracting membersâ€¦\")\n",
    "    sql = '''\n",
    "        SELECT \n",
    "            \"external_identifier\" AS patient_id,\n",
    "            \"firstName\", \"lastName\", \"gender\", \"birthDate\",\n",
    "            \"ethnicity\", \"createdAt\", \"updatedAt\"\n",
    "        FROM \"Member\"\n",
    "        WHERE \"birthDate\" IS NOT NULL\n",
    "          AND \"gender\" IS NOT NULL\n",
    "          AND \"external_identifier\" IS NOT NULL\n",
    "    '''\n",
    "    df = _read_sql(sql, CORE_ENGINE)\n",
    "    df = df.drop_duplicates(subset=[\"patient_id\"]).copy()\n",
    "    df[\"patient_id\"] = df[\"patient_id\"].astype(str)\n",
    "    print(f\"  âœ… Members: {len(df):,}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 2) ADT events â†’ patient_id\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def extract_adt_events(members_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    print(\"\\nðŸ¥ Extracting ADT eventsâ€¦\")\n",
    "    sql = '''\n",
    "        SELECT \n",
    "            evn.\"recordedDateTime\" AS event_datetime,\n",
    "            evn.\"eventTypeCode\"    AS event_type_code,\n",
    "            mast.\"memberId\"        AS member_id\n",
    "        FROM \"AdtEVN\" evn\n",
    "        JOIN \"AdmissionDischargeTransferMaster\" mast ON evn.\"adtMasterId\" = mast.id\n",
    "        WHERE evn.\"recordedDateTime\" BETWEEN '2022-01-01' AND '2024-12-31'\n",
    "          AND evn.\"eventTypeCode\" IN ('A01','A04','A08')\n",
    "        ORDER BY evn.\"recordedDateTime\"'''\n",
    "    ev = _read_sql(sql, CORE_ENGINE)\n",
    "    if ev.empty:\n",
    "        print(\"  âš ï¸ No ADT rows.\")\n",
    "        return pd.DataFrame(columns=[\"patient_id\",\"event_datetime\",\"eventType\"])\n",
    "\n",
    "    # Map to patient_id\n",
    "    mm = _member_map()\n",
    "    ev[\"member_id\"] = ev[\"member_id\"].astype(str)\n",
    "    ev = ev.merge(mm, on=\"member_id\", how=\"inner\")\n",
    "\n",
    "    # Normalize event category\n",
    "    def _cat(code):\n",
    "        if code in (\"A01\",\"A04\"): return \"admission\"\n",
    "        if code == \"A08\": return \"ed_visit\"\n",
    "        return \"other\"\n",
    "    ev[\"eventType\"] = ev[\"event_type_code\"].apply(_cat)\n",
    "    ev = ev[(ev[\"eventType\"].isin([\"admission\",\"ed_visit\"]))].copy()\n",
    "\n",
    "    # Coerce datetime and keep only members present in members_df\n",
    "    ev[\"event_datetime\"] = pd.to_datetime(ev[\"event_datetime\"], errors=\"coerce\")\n",
    "    ev = ev[ev[\"patient_id\"].isin(members_df[\"patient_id\"])].copy()\n",
    "    ev = ev.sort_values(\"event_datetime\").reset_index(drop=True)\n",
    "    print(f\"  âœ… ADT events: {len(ev):,} across {ev['patient_id'].nunique():,} patients\")\n",
    "    return ev[[\"patient_id\",\"event_datetime\",\"eventType\"]]\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 3) Engaged patients & occurred encounters (interventions)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def engaged_patient_ids() -> set[str]:\n",
    "    sql = 'SELECT \"id\" AS lighthouse_patient_id, \"status\" FROM \"Patient\" WHERE \"status\" IN ({})'.format(\n",
    "        \",\".join([f\"'{s}'\" for s in ENGAGED_STATUSES])\n",
    "    )\n",
    "    pats = _read_sql(sql, LH_ENGINE)\n",
    "    if pats.empty:\n",
    "        print(\"  âš ï¸ No engaged patients by status.\")\n",
    "        return set()\n",
    "    mm = _member_map()\n",
    "    pats[\"lighthouse_patient_id\"] = pats[\"lighthouse_patient_id\"].astype(str)\n",
    "    mm[\"member_id\"] = mm[\"member_id\"].astype(str)\n",
    "    joined = pats.merge(mm, left_on=\"lighthouse_patient_id\", right_on=\"member_id\", how=\"left\")\n",
    "    engaged = set(joined[\"patient_id\"].dropna().astype(str))\n",
    "    print(f\"  âœ… Engaged patients: {len(engaged):,}\")\n",
    "    return engaged\n",
    "\n",
    "\n",
    "def extract_interventions_occurred_engaged(engaged_ids: set[str]) -> pd.DataFrame:\n",
    "    print(\"\\nðŸ’Š Extracting occurred encounters for engaged patientsâ€¦\")\n",
    "    sql = '''\n",
    "        SELECT en.\"id\" AS encounter_id,\n",
    "               en.\"patientId\" AS lighthouse_patient_id,\n",
    "               en.\"dateOfEncounter\" AS encounter_datetime,\n",
    "               en.\"note\" AS encounter_note,\n",
    "               en.\"encounterOccurred\"\n",
    "        FROM \"EncounterNote\" en\n",
    "        WHERE en.\"published\" = true\n",
    "          AND en.\"deleted\"   = false\n",
    "          AND en.\"encounterOccurred\" = 'YES'\n",
    "        ORDER BY en.\"dateOfEncounter\"'''\n",
    "    en = _read_sql(sql, LH_ENGINE)\n",
    "    if en.empty:\n",
    "        print(\"  âš ï¸ No occurred encounters.\")\n",
    "        return pd.DataFrame(columns=[\"patient_id\",\"encounter_datetime\",\"encounter_note\"])\n",
    "\n",
    "    # Map lighthouse â†’ patient_id\n",
    "    mm = _member_map()\n",
    "    en[\"lighthouse_patient_id\"] = en[\"lighthouse_patient_id\"].astype(str)\n",
    "    mm[\"member_id\"] = mm[\"member_id\"].astype(str)\n",
    "    en = en.merge(mm, left_on=\"lighthouse_patient_id\", right_on=\"member_id\", how=\"left\")\n",
    "    en = en.dropna(subset=[\"patient_id\"]).copy()\n",
    "\n",
    "    # Keep only engaged\n",
    "    if engaged_ids:\n",
    "        en = en[en[\"patient_id\"].astype(str).isin(engaged_ids)].copy()\n",
    "\n",
    "    en[\"encounter_datetime\"] = pd.to_datetime(en[\"encounter_datetime\"], errors=\"coerce\")\n",
    "    out = en[[\"patient_id\",\"encounter_datetime\",\"encounter_note\"]].sort_values([\"patient_id\",\"encounter_datetime\"]).reset_index(drop=True)\n",
    "    print(f\"  âœ… Interventions kept: {len(out):,} across {out['patient_id'].nunique():,} patients\")\n",
    "    return out\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 4) Diagnoses free-text â†’ categories â†’ per-patient features\n",
    "# ---------------------------------------------------------------------------\n",
    "_DX_CATS = {\n",
    "    \"renal\":        [r\"\\brenal\\b\", r\"kidney\", r\"neph\"],\n",
    "    \"infection\":    [r\"infection\", r\"sepsis\", r\"uti\\b\", r\"pneumonia\", r\"cellulitis\", r\"abscess\"],\n",
    "    \"injury\":       [r\"injur\", r\"fracture\", r\"sprain\", r\"lacerat\", r\"contusion\", r\"burn\"],\n",
    "    \"respiratory\":  [r\"asthma\", r\"copd\", r\"respir\", r\"bronch\", r\"sob\", r\"dyspnea\"],\n",
    "    \"cardiac\":      [r\"card\", r\"mi\\b\", r\"myocard\", r\"afib\", r\"arrhythm\", r\"chf\", r\"heart\"],\n",
    "    \"neuro\":        [r\"neuro\", r\"stroke\", r\"cva\\b\", r\"seizure\", r\"tia\\b\", r\"migraine\", r\"headache\"],\n",
    "    \"psych\":        [r\"psych\", r\"depress\", r\"anx\", r\"bipolar\", r\"schizo\", r\"suicid\"],\n",
    "    \"pain\":         [r\"pain\", r\"ache\", r\"cramp\", r\"spasm\"],\n",
    "    \"pregnancy\":    [r\"pregnan\", r\"ob\\b\", r\"obstet\", r\"gyn\\b\", r\"miscar\", r\"prenatal\"],\n",
    "    \"gi\":           [r\"nausea\", r\"vomit|emesis\", r\"diarrhea\", r\"abdom|abd\\b|belly\", r\"gi\\b\", r\"gastr|ulcer\"],\n",
    "    \"gu\":           [r\"urinar|dysuria|hematur|pyel|prostat\", r\"gu\\b\"],\n",
    "    \"endocrine\":    [r\"diabet|hypergly|hypogly|thyroid|adrenal|endocr\"],\n",
    "    \"oncology\":     [r\"cancer|onc|tumor|malignan|neoplasm\"],\n",
    "    \"trauma\":       [r\"mvc|mva|gunshot|stab|assault|fall\\b|trauma\"],\n",
    "    \"skin_wound\":   [r\"rash|dermat|wound|ulcer|decub|psoria|eczema\"],\n",
    "    \"substance\":    [r\"etoh|alcohol|opioid|overdose|cocaine|meth|fentanyl|substance\"],\n",
    "    \"social\":       [r\"homeless|housing|transport|food|utility|childcare|violence|safety\"],\n",
    "    \"dental\":       [r\"dental|tooth|teeth|oral|abscess\"],\n",
    "    \"other\":        [r\"ekg|tbd|other|unspecified|unknown\"],\n",
    "}\n",
    "_DX_RX = {k: re.compile(\"|\".join(v), re.I) for k, v in _DX_CATS.items()}\n",
    "\n",
    "def build_dx_features() -> pd.DataFrame:\n",
    "    print(\"\\nðŸ§¬ Loading Diagnoses â†’ categoriesâ€¦\")\n",
    "    dx = _read_sql('SELECT \"patientId\",\"description\",\"createdAt\" FROM \"Diagnoses\"', LH_ENGINE)\n",
    "    if dx.empty:    # CRITICAL FIX: Apply temporal filtering to prevent data leakage\n",
    "    if dx.empty:    dx[\"createdAt\"] = pd.to_datetime(dx[\"createdAt\"], errors=\"coerce\")\n",
    "    if dx.empty:    dx = dx.dropna(subset=[\"createdAt\"]).copy()\n",
    "    if dx.empty:    dx = dx[dx[\"createdAt\"] < cutoff_date].copy()\n",
    "    if dx.empty:    \n",
    "    if dx.empty:\n",
    "        print(\"  âš ï¸ No Diagnoses rows.\")\n",
    "        return pd.DataFrame(columns=[\"patient_id\"])  # empty join\n",
    "\n",
    "    mm = _member_map()\n",
    "    dx[\"patientId\"] = dx[\"patientId\"].astype(str)\n",
    "    mm[\"member_id\"]  = mm[\"member_id\"].astype(str)\n",
    "    dx = dx.merge(mm, left_on=\"patientId\", right_on=\"member_id\", how=\"left\")\n",
    "    dx = dx.dropna(subset=[\"patient_id\"]).copy()\n",
    "\n",
    "    def cats_for(text: str) -> list[str]:\n",
    "        s = str(text) if isinstance(text, str) else \"\"\n",
    "        s = re.sub(r\"[\\\\/;|]+\", \",\", s)\n",
    "        hits = [k for k, rx in _DX_RX.items() if rx.search(s)]\n",
    "        return sorted(set(hits))\n",
    "\n",
    "    dx[\"_cats\"] = dx[\"description\"].apply(cats_for)\n",
    "    rows = []\n",
    "    for pid, cats in zip(dx[\"patient_id\"].astype(str), dx[\"_cats\"]):\n",
    "        for c in cats:\n",
    "            rows.append((pid, c))\n",
    "    if not rows:\n",
    "        print(\"  âš ï¸ No recognizable dx categories.\")\n",
    "        return pd.DataFrame(columns=[\"patient_id\"])  # empty join\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=[\"patient_id\",\"category\"])\\\n",
    "           .groupby([\"patient_id\",\"category\"]).size().reset_index(name=\"count\")\n",
    "    piv = df.pivot(index=\"patient_id\", columns=\"category\", values=\"count\").fillna(0).astype(int)\n",
    "    piv.columns = [f\"dx_cat_{c}_count\" for c in piv.columns]\n",
    "    flags = (piv > 0).astype(int)\n",
    "    flags.columns = [c.replace(\"_count\",\"_any\") for c in piv.columns]\n",
    "    out = pd.concat([piv, flags], axis=1).reset_index()\n",
    "    out[\"dx_any_count\"] = out[[c for c in out.columns if c.endswith(\"_count\")]].sum(axis=1)\n",
    "    print(f\"  âœ… Dx features built for {len(out):,} patients\")\n",
    "    return out\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 5) Leakâ€‘free features + outcomes\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def build_features(members: pd.DataFrame, adt: pd.DataFrame, interventions: pd.DataFrame, engaged_ids: set[str], dx_feat: pd.DataFrame) -> pd.DataFrame:\n",
    "    print(\"\\nðŸ§± Building leakâ€‘free features + outcomesâ€¦\")\n",
    "    feat = members.copy()\n",
    "\n",
    "    # Demographics\n",
    "    feat[\"birthDate\"] = pd.to_datetime(feat[\"birthDate\"], errors=\"coerce\")\n",
    "    feat = feat.dropna(subset=[\"birthDate\"]).copy()\n",
    "    feat[\"age_years\"] = (CUTOFF_DATE - feat[\"birthDate\"]).dt.days / 365.25\n",
    "    feat = feat[(feat[\"age_years\"] >= 0) & (feat[\"age_years\"] <= 120)].copy()\n",
    "    # Gender encoding tolerant to values like 'M'/'F' or 'Male'/'Female'\n",
    "    g = feat[\"gender\"].astype(str).str.upper().str[0]\n",
    "    feat[\"gender_male\"] = (g == \"M\").astype(int)\n",
    "    feat[\"gender_female\"] = (g == \"F\").astype(int)\n",
    "\n",
    "    # Utilization from historical ADT\n",
    "    adt_h = adt[adt[\"event_datetime\"] < CUTOFF_DATE].copy()\n",
    "    adt_h = adt_h[adt_h[\"patient_id\"].isin(feat[\"patient_id\"])].copy()\n",
    "    util = adt_h.groupby(\"patient_id\").agg(\n",
    "        total_events=(\"event_datetime\",\"count\"),\n",
    "        unique_event_days=(\"event_datetime\", lambda x: x.dt.date.nunique()),\n",
    "        admission_count=(\"eventType\", lambda s: (s == \"admission\").sum()),\n",
    "        ed_visit_count=(\"eventType\", lambda s: (s == \"ed_visit\").sum()),\n",
    "    ).reset_index()\n",
    "    feat = feat.merge(util, on=\"patient_id\", how=\"left\")\n",
    "    for c in [\"total_events\",\"unique_event_days\",\"admission_count\",\"ed_visit_count\"]:\n",
    "        feat[c] = feat[c].fillna(0).astype(int)\n",
    "\n",
    "    # Historical interventions count (occurred encounters)\n",
    "    if not interventions.empty:\n",
    "        inter_h = interventions[interventions[\"encounter_datetime\"] < CUTOFF_DATE].copy()\n",
    "        inter_cnt = inter_h.groupby(\"patient_id\").size().reset_index(name=\"intervention_history_count\")\n",
    "        feat = feat.merge(inter_cnt, on=\"patient_id\", how=\"left\")\n",
    "        feat[\"intervention_history_count\"] = feat[\"intervention_history_count\"].fillna(0).astype(int)\n",
    "    else:\n",
    "        feat[\"intervention_history_count\"] = 0\n",
    "\n",
    "    # is_engaged flag\n",
    "    feat[\"is_engaged\"] = feat[\"patient_id\"].astype(str).isin(engaged_ids)\n",
    "\n",
    "    # Risk score toy feature\n",
    "    feat[\"risk_score_basic\"] = (\n",
    "        0.3 * (feat[\"age_years\"] / 100.0) +\n",
    "        0.4 * np.log1p(feat[\"total_events\"]) +\n",
    "        0.3 * np.log1p(feat[\"intervention_history_count\"])\n",
    "    )\n",
    "\n",
    "    # Outcomes from future ADT windows\n",
    "    adt_f = adt[(adt[\"event_datetime\"] >= CUTOFF_DATE)].copy()\n",
    "    for w in (7, 30, 90, 180):\n",
    "        end = CUTOFF_DATE + pd.Timedelta(days=w)\n",
    "        win = adt_f[(adt_f[\"event_datetime\"] <= end)]\n",
    "        had = set(win[\"patient_id\"].unique())\n",
    "        feat[f\"outcome_{w}d\"] = feat[\"patient_id\"].isin(had).astype(int)\n",
    "        first = win.groupby(\"patient_id\")[\"event_datetime\"].min()\n",
    "        feat[f\"time_to_event_{w}d\"] = w\n",
    "        # assign observed times\n",
    "        idx = feat[\"patient_id\"].map(first).dropna().index\n",
    "        feat.loc[idx, f\"time_to_event_{w}d\"] = (\n",
    "            feat.loc[idx, \"patient_id\"].map(first) - CUTOFF_DATE\n",
    "        ).dt.days.values\n",
    "        print(f\"  â€¢ {w}d event rate: {feat[f'outcome_{w}d'].mean():.3f}\")\n",
    "\n",
    "    # Merge dx features\n",
    "    if not dx_feat.empty:\n",
    "        feat = feat.merge(dx_feat, on=\"patient_id\", how=\"left\")\n",
    "        for c in [c for c in feat.columns if c.startswith(\"dx_cat_\") or c == \"dx_any_count\"]:\n",
    "            feat[c] = feat[c].fillna(0)\n",
    "\n",
    "    print(f\"  âœ… Feature matrix shape: {feat.shape}\")\n",
    "    return feat\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 6) Optional signal_risks snapshot (Core)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def extract_signal_risks() -> pd.DataFrame:\n",
    "    try:\n",
    "        df = _read_sql('SELECT * FROM \"signal_risks\"', CORE_ENGINE)\n",
    "        if not df.empty:\n",
    "            out = OUTPUT_DIR/\"signal_risks.csv\"\n",
    "            df.to_csv(out, index=False)\n",
    "            print(f\"ðŸ’¾ Saved signal_risks â†’ {out}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"  âš ï¸ signal_risks load skipped: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 7) Runner\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def main():\n",
    "    print(\"ðŸš€ BLOCK 1 â€” FULL DATA BUILD\")\n",
    "    members = extract_members()\n",
    "    adt     = extract_adt_events(members)\n",
    "    engaged = engaged_patient_ids()\n",
    "    inter   = extract_interventions_occurred_engaged(engaged)\n",
    "    dx_feat = build_dx_features()\n",
    "\n",
    "    # Build features\n",
    "    features = build_features(members, adt, inter, engaged, dx_feat)\n",
    "\n",
    "    # Save outputs\n",
    "    out_fp = OUTPUT_DIR/\"feature_matrix.parquet\"\n",
    "    features.to_parquet(out_fp, index=False)\n",
    "    features.to_parquet(OUTPUT_DIR/\"features_df.parquet\", index=False)  # helper mirror\n",
    "    print(f\"ðŸ’¾ Saved feature matrix â†’ {out_fp}\")\n",
    "\n",
    "    # feature_cols for modeling\n",
    "    drop = set([\"patient_id\",\"firstName\",\"lastName\",\"birthDate\",\"gender\",\"ethnicity\",\"createdAt\",\"updatedAt\",\"is_engaged\"]) \\\n",
    "         | {f\"outcome_{w}d\" for w in (7,30,90,180)} \\\n",
    "         | {f\"time_to_event_{w}d\" for w in (7,30,90,180)}\n",
    "    feature_cols = [c for c in features.columns if c not in drop]\n",
    "    (OUTPUT_DIR/\"feature_meta.json\").write_text(json.dumps({\"feature_cols\": feature_cols}, indent=2))\n",
    "    (ART_DIR/\"feature_cols.json\").write_text(json.dumps({\"feature_cols\": feature_cols}, indent=2))\n",
    "    print(f\"ðŸ§© feature_cols count: {len(feature_cols)} (saved to feature_meta.json + artifacts/feature_cols.json)\")\n",
    "\n",
    "    # Save interventions (occurred, engaged) for downstream blocks\n",
    "    inter.to_csv(OUTPUT_DIR/\"interventions_df.csv\", index=False)\n",
    "    print(\"ðŸ’¾ Saved interventions â†’ publication_outputs/interventions_df.csv\")\n",
    "\n",
    "    # Optional debug exports\n",
    "    adt.to_csv(OUTPUT_DIR/\"adt_events_clean.csv\", index=False)\n",
    "\n",
    "    # signal_risks snapshot\n",
    "    extract_signal_risks()\n",
    "\n",
    "    # Run meta\n",
    "    meta = {\n",
    "        \"built_at\": datetime.utcnow().isoformat() + \"Z\",\n",
    "        \"cutoff_date\": str(CUTOFF_DATE.date()),\n",
    "        \"rows_features\": int(len(features)),\n",
    "        \"rows_adt\": int(len(adt)),\n",
    "        \"rows_interventions\": int(len(inter)),\n",
    "        \"engaged_count\": int(len(engaged)),\n",
    "    }\n",
    "    (OUTPUT_DIR/\"run_meta.json\").write_text(json.dumps(meta, indent=2))\n",
    "    print(\"\\nâœ… Block 1 complete â€” ready for Blocks 2â€“5.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69a316e-b01e-4db6-acba-eba012f90a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Miniâ€‘Block â€” Table 1 (Baseline Cohorts)\n",
    "=======================================\n",
    "Reads the feature matrix and optional interventions list, then writes\n",
    "baseline characteristics for (a) Overall TTE cohort, (b) Engaged subset,\n",
    "(c) Engaged & Treated subset.\n",
    "\n",
    "Inputs (from ./publication_outputs):\n",
    "  â€¢ feature_matrix.parquet (required)\n",
    "  â€¢ interventions_df.csv   (optional)\n",
    "\n",
    "Outputs (to ./publication_outputs):\n",
    "  â€¢ table1_overall_cohort.csv\n",
    "  â€¢ table1_engaged_subset.csv\n",
    "  â€¢ table1_engaged_treated_subset.csv\n",
    "  â€¢ table1_baseline_characteristics.xlsx (if xlsxwriter available)\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "OUT = Path(\"publication_outputs\"); OUT.mkdir(exist_ok=True)\n",
    "FEATURES_FP = OUT / \"feature_matrix.parquet\"\n",
    "INTERVENTIONS_FP = OUT / \"interventions_df.csv\"\n",
    "\n",
    "RISK_WINDOWS = (7, 30, 90, 180)\n",
    "\n",
    "\n",
    "def _must_exist(p: Path):\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Missing {p}. Run Block 1 first.\")\n",
    "\n",
    "\n",
    "def _pct(x):\n",
    "    x = float(x) if pd.notnull(x) else np.nan\n",
    "    return 100.0 * x\n",
    "\n",
    "\n",
    "def _cont_stats(s: pd.Series, name: str):\n",
    "    s = pd.to_numeric(s, errors=\"coerce\").dropna()\n",
    "    if s.empty:\n",
    "        return {f\"{name}_mean\": np.nan, f\"{name}_sd\": np.nan, f\"{name}_median\": np.nan,\n",
    "                f\"{name}_p25\": np.nan, f\"{name}_p75\": np.nan}\n",
    "    return {\n",
    "        f\"{name}_mean\": float(s.mean()),\n",
    "        f\"{name}_sd\": float(s.std(ddof=1)),\n",
    "        f\"{name}_median\": float(s.median()),\n",
    "        f\"{name}_p25\": float(s.quantile(0.25)),\n",
    "        f\"{name}_p75\": float(s.quantile(0.75)),\n",
    "    }\n",
    "\n",
    "\n",
    "def summarize(df: pd.DataFrame, label: str) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    N = len(df)\n",
    "    rows.append({\"Group\": label, \"Metric\": \"N\", \"Value\": N})\n",
    "\n",
    "    if \"age\" in df.columns:\n",
    "        rows.extend({\"Group\": label, \"Metric\": k, \"Value\": v} for k, v in _cont_stats(df[\"age\"], \"Age\").items())\n",
    "\n",
    "    if \"gender\" in df.columns:\n",
    "        g = df[\"gender\"].astype(str).str.upper()\n",
    "        rows.append({\"Group\": label, \"Metric\": \"Female_%\", \"Value\": _pct((g == \"F\").mean())})\n",
    "        rows.append({\"Group\": label, \"Metric\": \"Male_%\",   \"Value\": _pct((g == \"M\").mean())})\n",
    "\n",
    "    if \"is_engaged\" in df.columns:\n",
    "        rows.append({\"Group\": label, \"Metric\": \"Engaged_%\", \"Value\": _pct(df[\"is_engaged\"].mean())})\n",
    "    if \"treated_any\" in df.columns:\n",
    "        rows.append({\"Group\": label, \"Metric\": \"Any_Treatment_%\", \"Value\": _pct(df[\"treated_any\"].mean())})\n",
    "\n",
    "    for w in RISK_WINDOWS:\n",
    "        col = f\"outcome_{w}d\"\n",
    "        if col in df.columns:\n",
    "            rows.append({\"Group\": label, \"Metric\": f\"Outcome_{w}d_%\", \"Value\": _pct(df[col].mean())})\n",
    "\n",
    "    # Optional utilization features if present\n",
    "    for cand in [\"hist_ed_visits\", \"hist_admissions\", \"hist_util_total\"]:\n",
    "        if cand in df.columns:\n",
    "            rows.extend({\"Group\": label, \"Metric\": k, \"Value\": v}\n",
    "                        for k, v in _cont_stats(df[cand], cand).items())\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def run():\n",
    "    _must_exist(FEATURES_FP)\n",
    "    features = pd.read_parquet(FEATURES_FP)\n",
    "\n",
    "    # Build treated_any from file if count col missing\n",
    "    treated_ids = set()\n",
    "    if INTERVENTIONS_FP.exists():\n",
    "        try:\n",
    "            tdf = pd.read_csv(INTERVENTIONS_FP, usecols=[\"patient_id\"]) \n",
    "            treated_ids = set(tdf[\"patient_id\"].astype(str).unique())\n",
    "        except Exception:\n",
    "            treated_ids = set()\n",
    "\n",
    "    overall = features.copy()\n",
    "    if \"is_engaged\" not in overall.columns:\n",
    "        overall[\"is_engaged\"] = False\n",
    "\n",
    "    if \"intervention_history_count\" in overall.columns:\n",
    "        overall[\"treated_any\"] = (overall[\"intervention_history_count\"].fillna(0) > 0).astype(int)\n",
    "    else:\n",
    "        overall[\"treated_any\"] = overall[\"patient_id\"].astype(str).isin(treated_ids).astype(int)\n",
    "\n",
    "    engaged = overall[overall[\"is_engaged\"] == True].copy()\n",
    "    engaged_treated = engaged[engaged[\"treated_any\"] == 1].copy()\n",
    "\n",
    "    t1_overall = summarize(overall, \"Overall\")\n",
    "    t1_engaged = summarize(engaged, \"Engaged\")\n",
    "    t1_engaged_treated = summarize(engaged_treated, \"Engaged_Treated\")\n",
    "\n",
    "    t1_overall.to_csv(OUT/\"table1_overall_cohort.csv\", index=False)\n",
    "    t1_engaged.to_csv(OUT/\"table1_engaged_subset.csv\", index=False)\n",
    "    t1_engaged_treated.to_csv(OUT/\"table1_engaged_treated_subset.csv\", index=False)\n",
    "\n",
    "    # Optional Excel workbook\n",
    "    try:\n",
    "        with pd.ExcelWriter(OUT/\"table1_baseline_characteristics.xlsx\", engine=\"xlsxwriter\") as w:\n",
    "            t1_overall.to_excel(w, sheet_name=\"Overall\", index=False)\n",
    "            t1_engaged.to_excel(w, sheet_name=\"Engaged\", index=False)\n",
    "            t1_engaged_treated.to_excel(w, sheet_name=\"Engaged_Treated\", index=False)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Could not write Excel Table 1 (xlsxwriter missing?): {e}\")\n",
    "\n",
    "    print(\"âœ“ Table 1 written to publication_outputs/:\\n  - table1_overall_cohort.csv\\n  - table1_engaged_subset.csv\\n  - table1_engaged_treated_subset.csv\\n  - table1_baseline_characteristics.xlsx (if available)\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f676c501-4240-4ac4-94b0-78e208f61acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip wheel setuptools\n",
    "!pip install torchtuples==0.2.2 pycox==0.3.0\n",
    "# torch is already present per your logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4acbe6-cc96-4dd1-93e5-7f734958a9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "BLOCK 2 â€” TIMEâ€‘TOâ€‘EVENT MODELING (robust, fast, and extensible)\n",
    "===============================================================\n",
    "- Original classifier zoo retained (LogReg family, RF/ET/GB, optional XGBoost)\n",
    "- CoxPH (lifelines) with ridge penalization and guards\n",
    "- Survival addâ€‘ons: Random Survival Forest (sksurvâ†’lifelines fallback),\n",
    "  DeepSurv (pycox), DeepHit (pycox), and **XGBâ€‘AFT** (ultraâ€‘fast survival)\n",
    "- Computes ROC AUC + Youden's J* at each horizon; writes same outputs as before\n",
    "\n",
    "**Fixes in this version**\n",
    "- Restores missing `_stratified_cap` (caused NameError)\n",
    "- NaN/Infâ€‘safe survival training via `_sanitize_survival_data`\n",
    "- Stable perâ€‘sample interpolation `_interp1d_safe` (fixes \"fp and xp\" error)\n",
    "- DeepSurv/DeepHit use `model.fit(x, (t, e))` (no TupleDataset dependency)\n",
    "- Optional **XGBâ€‘AFT** engine to keep runs fast on large datasets\n",
    "\n",
    "Inputs (looked for in ./publication_outputs, with artifact fallbacks):\n",
    "  â€¢ publication_outputs/feature_matrix.parquet\n",
    "  â€¢ publication_outputs/feature_meta.json (or artifacts/feature_cols.json)\n",
    "\n",
    "Outputs (in ./publication_outputs):\n",
    "  â€¢ best_tte_model_predictions.csv (last window fit)\n",
    "  â€¢ best_tte_model_predictions_30d.csv\n",
    "  â€¢ table2_time_to_event_performance.csv\n",
    "  â€¢ tte_results.json\n",
    "  â€¢ complete_results_<timestamp>.json\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import json, shutil, sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GroupShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier,\n",
    ")\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "# Optional XGBoost (for classifiers and AFT survival)\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGBOOST_AVAILABLE = True\n",
    "except Exception:\n",
    "    XGBOOST_AVAILABLE = False\n",
    "\n",
    "# lifelines (CoxPH + RSF fallback)\n",
    "try:\n",
    "    from lifelines import CoxPHFitter\n",
    "    LIFELINES_AVAILABLE = True\n",
    "except Exception:\n",
    "    LIFELINES_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    from lifelines import RandomSurvivalForest as LL_RSF\n",
    "    LL_RSF_AVAILABLE = True\n",
    "except Exception:\n",
    "    LL_RSF_AVAILABLE = False\n",
    "\n",
    "# scikitâ€‘survival RSF (preferred)\n",
    "try:\n",
    "    from sksurv.ensemble import RandomSurvivalForest as SKS_RSF\n",
    "    from sksurv.util import Surv as SKS_Surv\n",
    "    SKS_AVAILABLE = True\n",
    "except Exception:\n",
    "    SKS_AVAILABLE = False\n",
    "\n",
    "# pycox (DeepSurv / DeepHit)\n",
    "try:\n",
    "    import torch\n",
    "    from torch import nn\n",
    "    import torchtuples as tt\n",
    "    from pycox.models import CoxPH as PYCoxPH\n",
    "    from pycox.models import DeepHitSingle\n",
    "    from pycox.preprocessing.label_transforms import LabTransDiscreteTime\n",
    "    PYCOX_AVAILABLE = True\n",
    "except Exception:\n",
    "    PYCOX_AVAILABLE = False\n",
    "\n",
    "if not PYCOX_AVAILABLE:\n",
    "    print(\"[Init] DeepSurv/DeepHit disabled: couldn't import pycox/torchtuples/torch.\")\n",
    "\n",
    "OUTPUT_DIR = Path(\"publication_outputs\"); OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "ART_DIR = Path(\"artifacts\")\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# =============================================================\n",
    "# Split configuration\n",
    "# =============================================================\n",
    "USE_TIME_SPLIT = True\n",
    "TIME_TEST_FRACTION = 0.20\n",
    "TIME_SPLIT_COLUMN = None\n",
    "TIME_SPLIT_DATE = None\n",
    "GROUP_BY_PATIENT = True\n",
    "PATIENT_ID_COL = \"patient_id\"\n",
    "\n",
    "# Metrics\n",
    "COMPUTE_JSTAR = True\n",
    "\n",
    "# =============================================================\n",
    "# FAST MODE toggles (edit for speed)\n",
    "# =============================================================\n",
    "ENABLE_SURVIVAL = True                 # master switch for survival addâ€‘ons\n",
    "DEVICE_OVERRIDE = None                 # set to \"cpu\" to avoid GPU warmup\n",
    "SURV_FAST_MODE = True\n",
    "MAX_TRAIN_SAMPLES_PER_WINDOW = 5000   # or smaller\n",
    "\n",
    "\n",
    "# Choose which survival engines to run\n",
    "# Options: 'rsf', 'deepsurv', 'deephit', 'xgb_aft'\n",
    "SURVIVAL_ENGINES = [\"rsf\", \"deepsurv\", \"deephit\", \"xgb_aft\"]\n",
    "#SURVIVAL_ENGINES = [\"xgb_aft\",\"rsf\"]    # skip deep nets entirely\n",
    "# Limit horizons survival runs are attempted on (keep all by default)\n",
    "SURVIVAL_WINDOWS = [7, 30, 90, 180]\n",
    "\n",
    "if SURV_FAST_MODE:\n",
    "    DEEPSURV_HIDDEN = [32, 32]\n",
    "    DEEPSURV_EPOCHS = 12\n",
    "    DEEPHIT_HIDDEN = [64, 64]\n",
    "    DEEPHIT_EPOCHS = 12\n",
    "    DEEPHIT_BINS = 40\n",
    "    RSF_TREES = 120\n",
    "else:\n",
    "    DEEPSURV_HIDDEN = [64, 64]\n",
    "    DEEPSURV_EPOCHS = 50\n",
    "    DEEPHIT_HIDDEN = [128, 128]\n",
    "    DEEPHIT_EPOCHS = 50\n",
    "    DEEPHIT_BINS = 100\n",
    "    RSF_TREES = 300\n",
    "\n",
    "# XGBâ€‘AFT parameters (fast survival)\n",
    "XGB_AFT_PARAMS = dict(\n",
    "    objective=\"survival:aft\",\n",
    "    eval_metric=\"aft-nloglik\",\n",
    "    aft_loss_distribution=\"normal\",     # also: 'logistic', 'extreme'\n",
    "    aft_loss_distribution_scale=1.20,\n",
    "    tree_method=\"hist\",\n",
    "    max_depth=6,\n",
    "    learning_rate=0.05,\n",
    "    reg_lambda=0.01,\n",
    "    reg_alpha=0.02,\n",
    "    verbosity=0,\n",
    ")\n",
    "XGB_AFT_NUM_ROUNDS = 400\n",
    "XGB_AFT_EARLY_STOP = 30\n",
    "\n",
    "# =============================================================\n",
    "# IO helpers\n",
    "# =============================================================\n",
    "\n",
    "def _mirror(src: Path, dst: Path):\n",
    "    dst.parent.mkdir(parents=True, exist_ok=True)\n",
    "    shutil.copy2(src, dst)\n",
    "    print(f\"ðŸ” Fallback: mirrored {src} â†’ {dst}\")\n",
    "\n",
    "def _load_features() -> pd.DataFrame:\n",
    "    for p in [OUTPUT_DIR/\"feature_matrix.parquet\", OUTPUT_DIR/\"features_df.parquet\", OUTPUT_DIR/\"features.parquet\"]:\n",
    "        if p.exists():\n",
    "            print(f\"Found features: {p}\")\n",
    "            return pd.read_parquet(p)\n",
    "    cands = sorted(ART_DIR.glob(\"features_master_*.parquet\")) if ART_DIR.exists() else []\n",
    "    if cands:\n",
    "        latest = cands[-1]\n",
    "        mirror = OUTPUT_DIR/\"feature_matrix.parquet\"\n",
    "        _mirror(latest, mirror)\n",
    "        return pd.read_parquet(mirror)\n",
    "    raise FileNotFoundError(\"Feature matrix not found in publication_outputs or artifacts.\")\n",
    "\n",
    "def _load_feature_cols(features: pd.DataFrame):\n",
    "    for p in [OUTPUT_DIR/\"feature_meta.json\", OUTPUT_DIR/\"feature_cols.json\", ART_DIR/\"feature_meta.json\", ART_DIR/\"feature_cols.json\"]:\n",
    "        p = Path(p)\n",
    "        if p.exists():\n",
    "            try:\n",
    "                meta = json.loads(p.read_text())\n",
    "                cols = meta.get(\"feature_cols\") or meta.get(\"features\")\n",
    "                if cols:\n",
    "                    print(f\"Loaded feature cols from {p}\")\n",
    "                    return cols\n",
    "            except Exception:\n",
    "                pass\n",
    "    # Infer if missing\n",
    "    drop = {\"patient_id\",\"firstName\",\"lastName\",\"birthDate\",\"gender\",\"ethnicity\",\"createdAt\",\"updatedAt\",\"has_notes\",\"engaged_only_flag\",\"DX_Summary\"}\n",
    "    for w in (7,30,90,180):\n",
    "        drop.add(f\"outcome_{w}d\"); drop.add(f\"time_to_event_{w}d\")\n",
    "    cols = [c for c in features.columns if c not in drop and pd.api.types.is_numeric_dtype(features[c])]\n",
    "    (OUTPUT_DIR/\"feature_meta.json\").write_text(json.dumps({\"feature_cols\": cols}, indent=2))\n",
    "    print(f\"Inferred {len(cols)} feature columns\")\n",
    "    return cols\n",
    "\n",
    "# =============================================================\n",
    "# Splitters\n",
    "# =============================================================\n",
    "\n",
    "def _pick_time_col(df: pd.DataFrame) -> str:\n",
    "    dt_cols = [c for c in df.columns if np.issubdtype(df[c].dtype, np.datetime64)]\n",
    "    candidates = dt_cols or [c for c in [\"as_of_date\",\"index_date\",\"feature_date\",\"cohort_date\",\"createdAt\",\"updatedAt\"] if c in df.columns]\n",
    "    if not candidates:\n",
    "        raise ValueError(\"No suitable time column found. Add one of: as_of_date/index_date/feature_date/cohort_date/createdAt/updatedAt, or set TIME_SPLIT_COLUMN explicitly.\")\n",
    "    return candidates[0]\n",
    "\n",
    "def _ensure_datetime(df: pd.DataFrame, col: str):\n",
    "    if not np.issubdtype(df[col].dtype, np.datetime64):\n",
    "        df[col] = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "def time_based_split(df: pd.DataFrame, date_col: str, test_fraction: float, split_date: str|None=None):\n",
    "    df = _ensure_datetime(df.copy(), date_col)\n",
    "    if df[date_col].isna().all():\n",
    "        raise ValueError(f\"Column {date_col} has no parseable datetimes. Provide a valid TIME_SPLIT_COLUMN or disable USE_TIME_SPLIT.\")\n",
    "    dates = df[date_col]\n",
    "    thr = pd.to_datetime(split_date) if split_date else dates.quantile(1 - test_fraction)\n",
    "    train_idx = dates <= thr\n",
    "    test_idx  = dates > thr\n",
    "    if train_idx.sum()==0 or test_idx.sum()==0:\n",
    "        thr = dates.sort_values().iloc[int((1 - test_fraction)*len(dates))]\n",
    "        train_idx = dates <= thr\n",
    "        test_idx  = dates > thr\n",
    "    print(f\"â±ï¸  Time split on {date_col}: train â‰¤ {thr.date()} | train={train_idx.sum()} test={test_idx.sum()}\")\n",
    "    return train_idx.values, test_idx.values, thr\n",
    "\n",
    "# =============================================================\n",
    "# Models & metrics\n",
    "# =============================================================\n",
    "\n",
    "def build_models():\n",
    "    models = {\n",
    "        \"LogisticRegression\": LogisticRegression(max_iter=2000, solver=\"lbfgs\", class_weight=\"balanced\", random_state=RANDOM_STATE),\n",
    "        \"ElasticNetLogistic\": LogisticRegression(max_iter=2000, solver=\"saga\", penalty=\"elasticnet\", l1_ratio=0.5, class_weight=\"balanced\", random_state=RANDOM_STATE),\n",
    "        \"RidgeLogistic\":      LogisticRegression(max_iter=2000, solver=\"lbfgs\", penalty=\"l2\", class_weight=\"balanced\", random_state=RANDOM_STATE),\n",
    "        \"LassoLogistic\":      LogisticRegression(max_iter=2000, solver=\"saga\", penalty=\"l1\", class_weight=\"balanced\", random_state=RANDOM_STATE),\n",
    "        \"RandomForest\":       RandomForestClassifier(n_estimators=400, max_depth=18, min_samples_leaf=10, class_weight=\"balanced_subsample\", random_state=RANDOM_STATE),\n",
    "        \"ExtraTrees\":         ExtraTreesClassifier(n_estimators=500, max_depth=18, min_samples_leaf=10, class_weight=\"balanced_subsample\", random_state=RANDOM_STATE),\n",
    "        \"GradientBoosting\":   GradientBoostingClassifier(random_state=RANDOM_STATE),\n",
    "    }\n",
    "    if XGBOOST_AVAILABLE:\n",
    "        models[\"XGBoost\"] = xgb.XGBClassifier(\n",
    "            n_estimators=600, max_depth=6, learning_rate=0.05,\n",
    "            subsample=0.8, colsample_bytree=0.8,\n",
    "            eval_metric=\"logloss\", random_state=RANDOM_STATE\n",
    "        )\n",
    "    return models\n",
    "\n",
    "\n",
    "def _auc_jstar(y_true, scores):\n",
    "    auc = float(roc_auc_score(y_true, scores)) if len(np.unique(y_true))>1 else np.nan\n",
    "    if not COMPUTE_JSTAR or np.isnan(auc):\n",
    "        return auc, np.nan\n",
    "    fpr, tpr, _ = roc_curve(y_true, scores)\n",
    "    J = tpr - fpr\n",
    "    jstar = float(J.max()) if len(J) else np.nan\n",
    "    return auc, jstar\n",
    "\n",
    "\n",
    "def _prep_design_matrix(df: pd.DataFrame, feature_cols):\n",
    "    X = df[feature_cols].copy()\n",
    "    nunique = X.nunique(dropna=False)\n",
    "    keep = nunique[nunique>1].index.tolist()\n",
    "    X = X[keep]\n",
    "    scaler = StandardScaler()\n",
    "    Xs = scaler.fit_transform(X.fillna(0.0).values)\n",
    "    return Xs, scaler, keep\n",
    "\n",
    "# =============================================================\n",
    "# Survival utilities (robust & fast)\n",
    "# =============================================================\n",
    "\n",
    "def _stratified_cap(X_trs, t_tr, e_tr, y_tr, cap):\n",
    "    \"\"\"Return a stratified (by event) subset if len(X_trs) > cap.\"\"\"\n",
    "    n = len(X_trs)\n",
    "    if cap is None or n <= cap:\n",
    "        return X_trs, t_tr, e_tr, y_tr\n",
    "    rng = np.random.RandomState(42)\n",
    "    pos_idx = np.flatnonzero(y_tr == 1)\n",
    "    neg_idx = np.flatnonzero(y_tr == 0)\n",
    "    pos_keep = min(len(pos_idx), cap // 2)\n",
    "    neg_keep = cap - pos_keep\n",
    "    pos_sel = rng.choice(pos_idx, size=pos_keep, replace=False) if pos_keep > 0 else np.array([], dtype=int)\n",
    "    neg_sel = rng.choice(neg_idx, size=neg_keep, replace=False)\n",
    "    sel = np.concatenate([pos_sel, neg_sel])\n",
    "    sel.sort()\n",
    "    return X_trs[sel], t_tr[sel], e_tr[sel], y_tr[sel]\n",
    "\n",
    "\n",
    "def _sanitize_survival_data(X, t, e, y, eps=1e-6):\n",
    "    \"\"\"Drop rows with NaN/Inf or negative t; clamp t to >= eps.\"\"\"\n",
    "    X = np.asarray(X, dtype=np.float32)\n",
    "    t = np.asarray(t, dtype=np.float32)\n",
    "    e = np.asarray(e, dtype=np.int32)\n",
    "    y = np.asarray(y, dtype=np.int32)\n",
    "    mask = np.isfinite(t) & (t >= 0) & np.isfinite(e) & np.isfinite(X).all(axis=1)\n",
    "    dropped = int(len(t) - mask.sum())\n",
    "    if dropped:\n",
    "        print(f\"    [sanitize] Dropped {dropped} rows with NaN/Inf/neg duration for survival training\")\n",
    "    t = np.clip(t[mask], eps, np.inf)\n",
    "    return X[mask], t, e[mask], y[mask]\n",
    "\n",
    "\n",
    "def _interp1d_safe(x, xp, fp, left, right):\n",
    "    \"\"\"np.interp guard: trims to common length, enforces monotone xp, returns float.\"\"\"\n",
    "    xp = np.asarray(xp, dtype=float)\n",
    "    fp = np.asarray(fp, dtype=float)\n",
    "    m = min(len(xp), len(fp))\n",
    "    if m == 0:\n",
    "        return float('nan')\n",
    "    xp = xp[:m]; fp = fp[:m]\n",
    "    xp = np.maximum.accumulate(xp)  # ensure nonâ€‘decreasing\n",
    "    return float(np.interp(float(x), xp, fp, left=left, right=right))\n",
    "\n",
    "# =============================================================\n",
    "# CoxPH helper (lifelines)\n",
    "# =============================================================\n",
    "\n",
    "def _coxph_fit_predict(train_df, test_df, feature_cols, duration_col, event_col):\n",
    "    if not LIFELINES_AVAILABLE:\n",
    "        raise RuntimeError(\"lifelines not available for CoxPH\")\n",
    "    X_tr = train_df[feature_cols].copy().fillna(0.0)\n",
    "    X_te = test_df[feature_cols].copy().fillna(0.0)\n",
    "    keep = X_tr.columns[X_tr.nunique(dropna=False)>1].tolist()\n",
    "    X_tr = X_tr[keep]; X_te = X_te[keep]\n",
    "    scaler = StandardScaler(); X_trs = scaler.fit_transform(X_tr.values); X_tes = scaler.transform(X_te.values)\n",
    "    tr = pd.DataFrame(X_trs, columns=keep, index=train_df.index)\n",
    "    te = pd.DataFrame(X_tes, columns=keep, index=test_df.index)\n",
    "    tr[duration_col] = train_df[duration_col].values\n",
    "    tr[event_col]    = train_df[event_col].values\n",
    "    cph = CoxPHFitter(penalizer=1.0)\n",
    "    try:\n",
    "        cph.fit(tr, duration_col=duration_col, event_col=event_col, robust=True)\n",
    "    except Exception:\n",
    "        cph = CoxPHFitter(penalizer=5.0)\n",
    "        cph.fit(tr, duration_col=duration_col, event_col=event_col, robust=True)\n",
    "    scores = cph.predict_partial_hazard(te).values.ravel()\n",
    "    return scores\n",
    "\n",
    "# =============================================================\n",
    "# DeepSurv / DeepHit / RSF / XGBâ€‘AFT\n",
    "# =============================================================\n",
    "\n",
    "def _deepsurv_fit_predict(X_trs, t_tr, e_tr, X_tes, horizon_days):\n",
    "    if not PYCOX_AVAILABLE:\n",
    "        raise RuntimeError(\"pycox not available for DeepSurv\")\n",
    "    input_dim = X_trs.shape[1]\n",
    "    layers, in_dim = [], input_dim\n",
    "    for h in DEEPSURV_HIDDEN:\n",
    "        layers += [nn.Linear(in_dim, h), nn.ReLU(), nn.BatchNorm1d(h), nn.Dropout(0.1)]\n",
    "        in_dim = h\n",
    "    layers += [nn.Linear(in_dim, 1)]\n",
    "    net = nn.Sequential(*layers)\n",
    "    device = DEVICE_OVERRIDE or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = PYCoxPH(net, tt.optim.Adam(lr=1e-3, weight_decay=1e-4), device=device)\n",
    "\n",
    "    x_tr = X_trs.astype(np.float32)\n",
    "    d_tr = np.clip(t_tr.astype(np.float32), 1e-6, np.inf)\n",
    "    e_trb = e_tr.astype(bool)\n",
    "\n",
    "    model.fit(x_tr, (d_tr, e_trb), batch_size=min(256, len(x_tr)), epochs=DEEPSURV_EPOCHS, verbose=False)\n",
    "    model.compute_baseline_hazards()\n",
    "\n",
    "    x_te = X_tes.astype(np.float32)\n",
    "    surv_df = model.predict_surv_df(x_te)  # pandas DataFrame (time index)\n",
    "    t_grid = surv_df.index.to_numpy(dtype=float)\n",
    "    S_all = surv_df.T.to_numpy()  # shape (n, len(t_grid))\n",
    "\n",
    "    S_w = np.empty(S_all.shape[0], dtype=float)\n",
    "    for i in range(S_all.shape[0]):\n",
    "        S_i = np.clip(S_all[i], 1e-12, 1.0)\n",
    "        S_w[i] = np.exp(_interp1d_safe(horizon_days, t_grid, np.log(S_i), left=np.log(0.999999), right=np.log(S_i[-1])))\n",
    "    return 1.0 - S_w\n",
    "\n",
    "\n",
    "def _deephit_fit_predict(X_trs, t_tr, e_tr, X_tes, horizon_days, num_durations=None):\n",
    "    if not PYCOX_AVAILABLE:\n",
    "        raise RuntimeError(\"pycox not available for DeepHit\")\n",
    "    num_durations = int(num_durations or DEEPHIT_BINS)\n",
    "    lab = LabTransDiscreteTime(num_durations)\n",
    "    yi, ye = lab.fit_transform(t_tr, e_tr)\n",
    "\n",
    "    input_dim = X_trs.shape[1]\n",
    "    layers, in_dim = [], input_dim\n",
    "    for h in DEEPHIT_HIDDEN:\n",
    "        layers += [nn.Linear(in_dim, h), nn.ReLU(), nn.BatchNorm1d(h), nn.Dropout(0.1)]\n",
    "        in_dim = h\n",
    "    layers += [nn.Linear(in_dim, num_durations)]\n",
    "    net = nn.Sequential(*layers)\n",
    "\n",
    "    device = DEVICE_OVERRIDE or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = DeepHitSingle(net, tt.optim.Adam(lr=1e-3), alpha=0.2, sigma=0.1, device=device)\n",
    "\n",
    "    x_tr = X_trs.astype(np.float32)\n",
    "    model.fit(x_tr, (yi, ye), batch_size=min(256, len(x_tr)), epochs=DEEPHIT_EPOCHS, verbose=False)\n",
    "\n",
    "    x_te = X_tes.astype(np.float32)\n",
    "    pmf = np.asarray(model.predict_pmf(x_te))  # shape (n, m)\n",
    "    surv_disc = np.cumprod(1 - pmf, axis=1)\n",
    "    cuts = lab.cuts  # length m+1\n",
    "    time_grid = cuts[1:].astype(float)  # length m\n",
    "\n",
    "    S_w = np.empty(surv_disc.shape[0], dtype=float)\n",
    "    for i in range(surv_disc.shape[0]):\n",
    "        S_i = np.clip(surv_disc[i], 1e-12, 1.0)\n",
    "        S_w[i] = _interp1d_safe(horizon_days, time_grid, S_i, left=1.0, right=S_i[-1])\n",
    "    return 1.0 - S_w\n",
    "\n",
    "\n",
    "def _rsf_fit_predict(X_trs, t_tr, e_tr, X_tes, horizon_days):\n",
    "    if SKS_AVAILABLE:\n",
    "        y_struct = SKS_Surv.from_arrays(event=e_tr.astype(bool), time=t_tr.astype(float))\n",
    "        rsf = SKS_RSF(n_estimators=RSF_TREES, min_samples_split=10, min_samples_leaf=5, random_state=17, n_jobs=-1, oob_score=True)\n",
    "        rsf.fit(X_trs, y_struct)\n",
    "        surv_funcs = rsf.predict_survival_function(X_tes, return_array=False)\n",
    "        out = np.zeros(len(X_tes), dtype=float)\n",
    "        for i, f in enumerate(surv_funcs):\n",
    "            out[i] = 1.0 - _interp1d_safe(horizon_days, f.x, f.y, left=0.0, right=1.0)\n",
    "        return out\n",
    "    if LL_RSF_AVAILABLE:\n",
    "        df_tr = pd.DataFrame(X_trs, columns=[f\"x{i}\" for i in range(X_trs.shape[1])])\n",
    "        df_tr[\"duration\"] = t_tr; df_tr[\"event\"] = e_tr\n",
    "        rsf = LL_RSF(n_estimators=RSF_TREES, min_samples_split=10, min_samples_leaf=5, random_state=17, n_jobs=-1)\n",
    "        rsf.fit(df_tr, duration_col=\"duration\", event_col=\"event\")\n",
    "        surv = rsf.predict_survival_function(X_tes)\n",
    "        out = np.zeros(len(X_tes), dtype=float)\n",
    "        for i, s in enumerate(surv):\n",
    "            t_grid = s.index.to_numpy(dtype=float); y = s.to_numpy(dtype=float)\n",
    "            out[i] = 1.0 - _interp1d_safe(horizon_days, t_grid, y, left=0.0, right=1.0)\n",
    "        return out\n",
    "    raise RuntimeError(\"No RSF backend available. Install 'scikit-survival' or 'lifelines'.\")\n",
    "\n",
    "\n",
    "def _xgb_aft_fit_predict_time(X_trs, t_tr, e_tr, X_tes,\n",
    "                              params=None, num_boost_round=None, early_stopping_rounds=None):\n",
    "    if not XGBOOST_AVAILABLE:\n",
    "        raise RuntimeError(\"XGBoost not available for AFT.\")\n",
    "    params = dict(XGB_AFT_PARAMS if params is None else params)\n",
    "    rnds = XGB_AFT_NUM_ROUNDS if num_boost_round is None else num_boost_round\n",
    "    es = XGB_AFT_EARLY_STOP if early_stopping_rounds is None else early_stopping_rounds\n",
    "\n",
    "    # Build ranged labels for AFT\n",
    "    y_lb = t_tr.astype(float)\n",
    "    y_ub = np.where(e_tr.astype(bool), t_tr.astype(float), np.inf)\n",
    "\n",
    "    dfull = xgb.DMatrix(X_trs)\n",
    "    dfull.set_float_info('label_lower_bound', y_lb)\n",
    "    dfull.set_float_info('label_upper_bound', y_ub)\n",
    "\n",
    "    # Simple holdout for early stopping\n",
    "    n = len(X_trs)\n",
    "    k = max(1000, min(n // 5, 5000)) if n > 2000 else max(100, n // 5)\n",
    "    idx_valid = np.arange(n - k, n)\n",
    "    idx_train = np.arange(0, n - k)\n",
    "\n",
    "    dtr = xgb.DMatrix(X_trs[idx_train])\n",
    "    dtr.set_float_info('label_lower_bound', y_lb[idx_train])\n",
    "    dtr.set_float_info('label_upper_bound', y_ub[idx_train])\n",
    "\n",
    "    dval = xgb.DMatrix(X_trs[idx_valid])\n",
    "    dval.set_float_info('label_lower_bound', y_lb[idx_valid])\n",
    "    dval.set_float_info('label_upper_bound', y_ub[idx_valid])\n",
    "\n",
    "    bst = xgb.train(params, dtr, num_boost_round=rnds, evals=[(dtr, 'train'), (dval, 'valid')],\n",
    "                    early_stopping_rounds=es, verbose_eval=False)\n",
    "\n",
    "    dtest = xgb.DMatrix(X_tes)\n",
    "    t_pred = bst.predict(dtest)   # monotone with time; use -t_pred as risk\n",
    "    return t_pred\n",
    "\n",
    "# =============================================================\n",
    "# Runner\n",
    "# =============================================================\n",
    "\n",
    "def main():\n",
    "    print(\"[Block 2] Loading featuresâ€¦\")\n",
    "    features = _load_features()\n",
    "\n",
    "    date_col = TIME_SPLIT_COLUMN or (_pick_time_col(features) if USE_TIME_SPLIT else None)\n",
    "    if USE_TIME_SPLIT:\n",
    "        features = _ensure_datetime(features, date_col)\n",
    "\n",
    "    feature_cols = _load_feature_cols(features)\n",
    "    print(\"Loaded feature cols from publication_outputs/feature_meta.json\" if (OUTPUT_DIR/\"feature_meta.json\").exists() else \"Inferred feature cols.\")\n",
    "\n",
    "    windows = [7, 30, 90, 180]\n",
    "    results = []\n",
    "\n",
    "    for w in windows:\n",
    "        print(f\"\\nTraining models for {w}-day horizonâ€¦\")\n",
    "        y_col = f\"outcome_{w}d\"; d_col = f\"time_to_event_{w}d\"\n",
    "        if y_col not in features.columns or d_col not in features.columns:\n",
    "            print(f\"  [WARN] Missing columns for {w}d â€” skipping\")\n",
    "            continue\n",
    "\n",
    "        df = features.dropna(subset=[y_col, d_col]).copy()\n",
    "        y = df[y_col].astype(int).values.ravel()\n",
    "\n",
    "        # Build time/random split indices\n",
    "        if USE_TIME_SPLIT:\n",
    "            train_mask, test_mask, thr = time_based_split(df, date_col, TIME_TEST_FRACTION, TIME_SPLIT_DATE)\n",
    "        else:\n",
    "            if GROUP_BY_PATIENT and PATIENT_ID_COL in df.columns:\n",
    "                gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=RANDOM_STATE)\n",
    "                tr_idx, te_idx = next(gss.split(df, groups=df[PATIENT_ID_COL]))\n",
    "                train_mask = np.zeros(len(df), dtype=bool); train_mask[tr_idx] = True\n",
    "                test_mask  = np.zeros(len(df), dtype=bool);  test_mask[te_idx]  = True\n",
    "                print(f\"ðŸ‘¥ Grouped random split by {PATIENT_ID_COL}: train={train_mask.sum()} test={test_mask.sum()}\")\n",
    "            else:\n",
    "                strat = y if np.unique(y).size>1 else None\n",
    "                tr, te = train_test_split(np.arange(len(df)), test_size=0.2, random_state=RANDOM_STATE, stratify=strat)\n",
    "                train_mask = np.zeros(len(df), dtype=bool); train_mask[tr] = True\n",
    "                test_mask  = np.zeros(len(df), dtype=bool);  test_mask[te]  = True\n",
    "                print(f\"ðŸŽ² Random split: train={train_mask.sum()} test={test_mask.sum()}\")\n",
    "\n",
    "        df_tr = df.loc[train_mask].copy(); df_te = df.loc[test_mask].copy()\n",
    "        y_tr = df_tr[y_col].astype(int).values.ravel(); y_te = df_te[y_col].astype(int).values.ravel()\n",
    "\n",
    "        # Design matrix (scale on TRAIN)\n",
    "        X_trs, scaler, keep_cols = _prep_design_matrix(df_tr, feature_cols)\n",
    "        X_tes = scaler.transform(df_te[keep_cols].fillna(0.0).values)\n",
    "\n",
    "        # ------- Existing classifier zoo -------\n",
    "        models = build_models()\n",
    "        if LIFELINES_AVAILABLE:\n",
    "            models = {**models, \"CoxPH\": None}  # placeholder; handled separately\n",
    "\n",
    "        best_name, best_auc = None, -np.inf\n",
    "        preds_for_best = None\n",
    "\n",
    "        for name, model in models.items():\n",
    "            try:\n",
    "                if name == \"CoxPH\":\n",
    "                    try:\n",
    "                        scores = _coxph_fit_predict(df_tr[[*keep_cols, d_col, y_col]], df_te[[*keep_cols, d_col, y_col]], keep_cols, duration_col=d_col, event_col=y_col)\n",
    "                        auc, jstar = _auc_jstar(y_te, scores)\n",
    "                        print(f\"  [{w}d] CoxPH             AUC={auc:.3f}  J*={(jstar if not np.isnan(jstar) else 0):.3f}\")\n",
    "                        y_scores = scores\n",
    "                    except Exception as ex:\n",
    "                        print(f\"  [{w}d] CoxPH             FAILED â€” {ex}\")\n",
    "                        continue\n",
    "                else:\n",
    "                    if isinstance(model, (RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier)):\n",
    "                        model = CalibratedClassifierCV(model, method=\"sigmoid\", cv=3)\n",
    "                    model.fit(X_trs, y_tr)\n",
    "                    y_scores = model.predict_proba(X_tes)[:,1]\n",
    "                    auc, jstar = _auc_jstar(y_te, y_scores)\n",
    "                    print(f\"  [{w}d] {name:<18} AUC={auc:.3f}  J*={(jstar if not np.isnan(jstar) else 0):.3f}\")\n",
    "                if auc > best_auc:\n",
    "                    best_auc = auc; best_name = name; preds_for_best = y_scores\n",
    "            except Exception as e:\n",
    "                print(f\"  [{w}d] {name:<18} FAILED â€” {e}\")\n",
    "                continue\n",
    "\n",
    "        # ------- Survival models (risk at horizon = 1 - S_w) -------\n",
    "        t_tr = df_tr[d_col].astype(float).values.ravel()\n",
    "        e_tr = df_tr[y_col].astype(int).values.ravel()\n",
    "        horizon = float(w)\n",
    "\n",
    "        if not ENABLE_SURVIVAL:\n",
    "            print(f\"  [{w}d] Survival models SKIPPED â€” ENABLE_SURVIVAL=False\")\n",
    "        else:\n",
    "            if w not in SURVIVAL_WINDOWS:\n",
    "                print(f\"  [{w}d] Survival models SKIPPED â€” not in SURVIVAL_WINDOWS={SURVIVAL_WINDOWS}\")\n",
    "            else:\n",
    "                X_cap, t_cap, e_cap, y_cap = _stratified_cap(X_trs, t_tr, e_tr, y_tr, MAX_TRAIN_SAMPLES_PER_WINDOW)\n",
    "                X_cap, t_cap, e_cap, y_cap = _sanitize_survival_data(X_cap, t_cap, e_cap, y_cap)\n",
    "\n",
    "                # DeepSurv\n",
    "                if \"deepsurv\" in SURVIVAL_ENGINES and PYCOX_AVAILABLE:\n",
    "                    try:\n",
    "                        scores = _deepsurv_fit_predict(X_cap, t_cap, e_cap, X_tes, horizon)\n",
    "                        auc, jstar = _auc_jstar(y_te, scores)\n",
    "                        print(f\"  [{w}d] DeepSurv          AUC={auc:.3f}  J*={(jstar if not np.isnan(jstar) else 0):.3f}\")\n",
    "                        if auc > best_auc:\n",
    "                            best_auc = auc; best_name = \"DeepSurv\"; preds_for_best = scores\n",
    "                    except Exception as e:\n",
    "                        print(f\"  [{w}d] DeepSurv          FAILED â€” {e}\")\n",
    "                elif \"deepsurv\" in SURVIVAL_ENGINES and not PYCOX_AVAILABLE:\n",
    "                    print(f\"  [{w}d] DeepSurv          SKIPPED â€” pycox/torchtuples not available\")\n",
    "\n",
    "                # DeepHit\n",
    "                if \"deephit\" in SURVIVAL_ENGINES and PYCOX_AVAILABLE:\n",
    "                    try:\n",
    "                        scores = _deephit_fit_predict(X_cap, t_cap, e_cap, X_tes, horizon, num_durations=DEEPHIT_BINS)\n",
    "                        auc, jstar = _auc_jstar(y_te, scores)\n",
    "                        print(f\"  [{w}d] DeepHit           AUC={auc:.3f}  J*={(jstar if not np.isnan(jstar) else 0):.3f}\")\n",
    "                        if auc > best_auc:\n",
    "                            best_auc = auc; best_name = \"DeepHit\"; preds_for_best = scores\n",
    "                    except Exception as e:\n",
    "                        print(f\"  [{w}d] DeepHit           FAILED â€” {e}\")\n",
    "                elif \"deephit\" in SURVIVAL_ENGINES and not PYCOX_AVAILABLE:\n",
    "                    print(f\"  [{w}d] DeepHit           SKIPPED â€” pycox/torchtuples not available\")\n",
    "\n",
    "                # RSF\n",
    "                if \"rsf\" in SURVIVAL_ENGINES and (SKS_AVAILABLE or LL_RSF_AVAILABLE):\n",
    "                    try:\n",
    "                        scores = _rsf_fit_predict(X_cap, t_cap, e_cap, X_tes, horizon)\n",
    "                        auc, jstar = _auc_jstar(y_te, scores)\n",
    "                        print(f\"  [{w}d] RSF               AUC={auc:.3f}  J*={(jstar if not np.isnan(jstar) else 0):.3f}\")\n",
    "                        if auc > best_auc:\n",
    "                            best_auc = auc; best_name = \"RSF\"; preds_for_best = scores\n",
    "                    except Exception as e:\n",
    "                        print(f\"  [{w}d] RSF               FAILED â€” {e}\")\n",
    "                elif \"rsf\" in SURVIVAL_ENGINES:\n",
    "                    print(f\"  [{w}d] RSF               SKIPPED â€” install scikit-survival or lifelines >=0.27\")\n",
    "\n",
    "                # XGBâ€‘AFT (ultraâ€‘fast)\n",
    "                if \"xgb_aft\" in SURVIVAL_ENGINES and XGBOOST_AVAILABLE:\n",
    "                    try:\n",
    "                        t_pred = _xgb_aft_fit_predict_time(X_cap, t_cap, e_cap, X_tes)\n",
    "                        scores = -t_pred  # smaller predicted time => higher risk\n",
    "                        auc, jstar = _auc_jstar(y_te, scores)\n",
    "                        print(f\"  [{w}d] XGBâ€‘AFT           AUC={auc:.3f}  J*={(jstar if not np.isnan(jstar) else 0):.3f}\")\n",
    "                        if auc > best_auc:\n",
    "                            best_auc = auc; best_name = \"XGBâ€‘AFT\"; preds_for_best = scores\n",
    "                    except Exception as e:\n",
    "                        print(f\"  [{w}d] XGBâ€‘AFT           FAILED â€” {e}\")\n",
    "                elif \"xgb_aft\" in SURVIVAL_ENGINES and not XGBOOST_AVAILABLE:\n",
    "                    print(f\"  [{w}d] XGBâ€‘AFT           SKIPPED â€” XGBoost not available\")\n",
    "\n",
    "        print(f\"â†’ Best @ {w}d: {best_name} (AUC {best_auc:.3f})\")\n",
    "        results.append({\"window\": w, \"best_model\": best_name, \"auc\": float(best_auc)})\n",
    "\n",
    "        # Persist perâ€‘patient predictions for downstream blocks\n",
    "        out = pd.DataFrame({\n",
    "            PATIENT_ID_COL: df_te[PATIENT_ID_COL].values,\n",
    "            \"risk_score\": preds_for_best,\n",
    "            f\"outcome_{w}d\": y_te.astype(int),\n",
    "        })\n",
    "        out.to_csv(OUTPUT_DIR/\"best_tte_model_predictions.csv\", index=False)\n",
    "        if w == 30:\n",
    "            out.to_csv(OUTPUT_DIR/\"best_tte_model_predictions_30d.csv\", index=False)\n",
    "\n",
    "    # Table 2 + JSONs\n",
    "    pd.DataFrame(results).to_csv(OUTPUT_DIR/\"table2_time_to_event_performance.csv\", index=False)\n",
    "    Path(OUTPUT_DIR/\"tte_results.json\").write_text(json.dumps(results, indent=2))\n",
    "    stamp = datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    Path(OUTPUT_DIR/f\"complete_results_{stamp}.json\").write_text(json.dumps({\n",
    "        \"config\": {\n",
    "            \"USE_TIME_SPLIT\": USE_TIME_SPLIT,\n",
    "            \"TIME_SPLIT_COLUMN\": TIME_SPLIT_COLUMN,\n",
    "            \"TIME_SPLIT_DATE\": TIME_SPLIT_DATE,\n",
    "            \"TIME_TEST_FRACTION\": TIME_TEST_FRACTION,\n",
    "            \"GROUP_BY_PATIENT\": GROUP_BY_PATIENT,\n",
    "            \"PATIENT_ID_COL\": PATIENT_ID_COL,\n",
    "            \"ENABLE_SURVIVAL\": ENABLE_SURVIVAL,\n",
    "            \"SURV_FAST_MODE\": SURV_FAST_MODE,\n",
    "            \"MAX_TRAIN_SAMPLES_PER_WINDOW\": MAX_TRAIN_SAMPLES_PER_WINDOW,\n",
    "            \"SURVIVAL_ENGINES\": SURVIVAL_ENGINES,\n",
    "            \"SURVIVAL_WINDOWS\": SURVIVAL_WINDOWS,\n",
    "        },\n",
    "        \"windows\": results\n",
    "    }, indent=2))\n",
    "\n",
    "    print(\"\\nâœ“ Block 2 complete â€” outputs:\")\n",
    "    print(\"  â€¢ best_tte_model_predictions.csv\")\n",
    "    print(\"  â€¢ best_tte_model_predictions_30d.csv (if 30d available)\")\n",
    "    print(\"  â€¢ table2_time_to_event_performance.csv\")\n",
    "    print(\"  â€¢ tte_results.json\")\n",
    "    print(\"  â€¢ complete_results_*.json\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8365b9a-56dd-42a8-a0d8-7220a8c9273b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "BLOCK 3 â€” INTERVENTION SELECTION (OPE publishâ€‘v10)\n",
    "==================================================\n",
    "- Engagedâ€‘only policy modeling & offâ€‘policy evaluation\n",
    "- **MODIFIED: Figure 2A now plots both ARR vs. Watchful Waiting and ARR vs. Status Quo.**\n",
    "- **MODIFIED: Figure 2A styling updated for clarity (no CI, no f0 bar).**\n",
    "- Explicitly selects 'CausalForest' for Figure 2 and best-model reporting.\n",
    "- Calculates ARR and NNT vs. Status Quo (observed rate).\n",
    "- Robust propensity (calibrated LR) with singleâ€‘class fallback.\n",
    "- Proper S/T/X/DR/R learners (+ DRForest/ExtraTrees/DRXGBoost opt).\n",
    "- Adaptive overlap trim with minimum sample requirement.\n",
    "- Weight clipping for IPS/DR (p99 by default) + ESS diagnostics.\n",
    "- AUUC (DR uplift) + balance diagnostics (SMD).\n",
    "\n",
    "Inputs (mirrored from ./artifacts if missing):\n",
    "  â€¢ publication_outputs/feature_matrix.parquet\n",
    "  â€¢ publication_outputs/interventions_df.csv (optional)\n",
    "  â€¢ publication_outputs/feature_meta.json (or artifacts/feature_cols.json)\n",
    "Outputs (in ./publication_outputs):\n",
    "  â€¢ table3_intervention_ope.csv\n",
    "  â€¢ figure2_capacity_curve.png (Now shows dual ARR plot)\n",
    "  â€¢ figure2_qini_uplift.png\n",
    "  â€¢ capacity_curve_best.csv\n",
    "  â€¢ capacity_sensitivity.csv\n",
    "  â€¢ balance_table.csv\n",
    "  â€¢ best_intervention_model_predictions.csv\n",
    "  â€¢ best_intervention_model_predictions_fulltest.csv\n",
    "  â€¢ intervention_ope_diagnostics.json\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import json, shutil\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, cohen_kappa_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, ExtraTreesClassifier,\n",
    "    RandomForestRegressor, ExtraTreesRegressor,\n",
    ")\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin, clone\n",
    "\n",
    "# Optional XGBoost\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGBOOST_AVAILABLE = True\n",
    "except Exception:\n",
    "    XGBOOST_AVAILABLE = False\n",
    "\n",
    "OUTPUT_DIR = Path(\"publication_outputs\"); OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "ART_DIR = Path(\"artifacts\")\n",
    "RANDOM_STATE = 42\n",
    "HORIZON_DAYS = 30\n",
    "EPS = 1e-6\n",
    "\n",
    "# =============================================================\n",
    "# Resilient loaders\n",
    "# =============================================================\n",
    "\n",
    "def _mirror(src: Path, dst: Path):\n",
    "    dst.parent.mkdir(parents=True, exist_ok=True)\n",
    "    shutil.copy2(src, dst)\n",
    "    print(f\"ðŸ” Fallback: mirrored {src} â†’ {dst}\")\n",
    "\n",
    "\n",
    "def _load_features() -> pd.DataFrame:\n",
    "    for p in [OUTPUT_DIR/\"feature_matrix.parquet\", OUTPUT_DIR/\"features_df.parquet\", OUTPUT_DIR/\"features.parquet\"]:\n",
    "        if p.exists():\n",
    "            print(f\"Found features: {p}\")\n",
    "            return pd.read_parquet(p)\n",
    "    cands = sorted(ART_DIR.glob(\"features_master_*.parquet\")) if ART_DIR.exists() else []\n",
    "    if cands:\n",
    "        latest = cands[-1]\n",
    "        mirror = OUTPUT_DIR/\"feature_matrix.parquet\"\n",
    "        _mirror(latest, mirror)\n",
    "        return pd.read_parquet(mirror)\n",
    "    raise FileNotFoundError(\"Feature matrix not found in publication_outputs or artifacts.\")\n",
    "\n",
    "\n",
    "def _load_interventions() -> pd.DataFrame:\n",
    "    for p in [OUTPUT_DIR/\"interventions_df.csv\", OUTPUT_DIR/\"interventions_processed.csv\"]:\n",
    "        if p.exists():\n",
    "            print(f\"Found interventions: {p}\")\n",
    "            return pd.read_csv(p)\n",
    "    cands = sorted(ART_DIR.glob(\"interventions_processed_*.csv\")) if ART_DIR.exists() else []\n",
    "    if cands:\n",
    "        latest = cands[-1]\n",
    "        mirror = OUTPUT_DIR/\"interventions_df.csv\"\n",
    "        _mirror(latest, mirror)\n",
    "        return pd.read_csv(mirror)\n",
    "    print(\"âš ï¸ No interventions file found. Proceeding without notesâ€‘based treatment inference.\")\n",
    "    return pd.DataFrame(columns=[\"patient_id\"])\n",
    "\n",
    "\n",
    "def _load_feature_cols(features: pd.DataFrame):\n",
    "    for p in [OUTPUT_DIR/\"feature_meta.json\", OUTPUT_DIR/\"feature_cols.json\", ART_DIR/\"feature_meta.json\", ART_DIR/\"feature_cols.json\"]:\n",
    "        p = Path(p)\n",
    "        if p.exists():\n",
    "            try:\n",
    "                meta = json.loads(p.read_text())\n",
    "                cols = meta.get(\"feature_cols\") or meta.get(\"features\")\n",
    "                if cols:\n",
    "                    print(f\"Loaded feature cols from {p}\")\n",
    "                    return cols\n",
    "            except Exception:\n",
    "                pass\n",
    "    drop = {\"patient_id\",\"firstName\",\"lastName\",\"birthDate\",\"gender\",\"ethnicity\",\"createdAt\",\"updatedAt\",\"has_notes\",\"engaged_only_flag\",\"DX_Summary\"}\n",
    "    for w in (7,30,90,180):\n",
    "        drop.add(f\"outcome_{w}d\"); drop.add(f\"time_to_event_{w}d\")\n",
    "    cols = [c for c in features.columns if c not in drop and pd.api.types.is_numeric_dtype(features[c])]\n",
    "    (OUTPUT_DIR/\"feature_meta.json\").write_text(json.dumps({\"feature_cols\": cols}, indent=2))\n",
    "    print(f\"Inferred {len(cols)} feature columns\")\n",
    "    return cols\n",
    "\n",
    "# =============================================================\n",
    "# Data prep (engagedâ€‘only)\n",
    "# =============================================================\n",
    "\n",
    "def prepare_intervention_frame(features: pd.DataFrame, interventions: pd.DataFrame):\n",
    "    \"\"\"Return standardized train/test arrays and metadata (engagedâ€‘only if available).\"\"\"\n",
    "    target_col = f\"outcome_{HORIZON_DAYS}d\"\n",
    "    if target_col not in features.columns:\n",
    "        raise ValueError(f\"Missing target column {target_col} in features.\")\n",
    "\n",
    "    if \"is_engaged\" in features.columns:\n",
    "        features = features[features[\"is_engaged\"] == True].copy()\n",
    "        print(f\"  Engagedâ€‘only subset: n={len(features)}\")\n",
    "\n",
    "    if \"intervention_history_count\" in features.columns:\n",
    "        treatment = (features[\"intervention_history_count\"].fillna(0) > 0).astype(int)\n",
    "    else:\n",
    "        treated_ids = set(interventions.get(\"patient_id\", pd.Series(dtype=str)).dropna().astype(str))\n",
    "        treatment = features[\"patient_id\"].astype(str).isin(treated_ids).astype(int)\n",
    "\n",
    "    feat_cols = _load_feature_cols(features)\n",
    "    X_df = features[feat_cols].copy().fillna(0.0)\n",
    "    y = features[target_col].astype(int).values.ravel()\n",
    "    t = treatment.astype(int).values.ravel()\n",
    "\n",
    "    stratify_vec = t if np.unique(t).size > 1 else None\n",
    "    X_tr_df, X_te_df, y_tr, y_te, t_tr, t_te = train_test_split(\n",
    "        X_df, y, t, test_size=0.2, random_state=RANDOM_STATE, stratify=stratify_vec\n",
    "    )\n",
    "\n",
    "    scaler = StandardScaler(); X_tr = scaler.fit_transform(X_tr_df.values); X_te = scaler.transform(X_te_df.values)\n",
    "    te_ids = features.loc[X_te_df.index, \"patient_id\"].values\n",
    "\n",
    "    print(f\"  Treatment rate â€” overall: {t.mean():.3f} | train: {t_tr.mean():.3f} | test: {t_te.mean():.3f}\")\n",
    "    meta = {\"feature_cols\": feat_cols}\n",
    "    return X_tr, X_te, np.asarray(y_tr).ravel(), np.asarray(y_te).ravel(), np.asarray(t_tr).ravel(), np.asarray(t_te).ravel(), te_ids, meta\n",
    "\n",
    "# =============================================================\n",
    "# Uplift model base + learners\n",
    "# =============================================================\n",
    "\n",
    "class _UpliftBase:\n",
    "    def mu0(self, X): raise NotImplementedError\n",
    "    def mu1(self, X): raise NotImplementedError\n",
    "    def uplift(self, X): return self.mu1(X) - self.mu0(X)\n",
    "    def benefit(self, X): return self.mu0(X) - self.mu1(X)\n",
    "    def policy(self, X): return (self.mu1(X) < self.mu0(X)).astype(int)\n",
    "\n",
    "class SLearner(_UpliftBase, BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, base=None):\n",
    "        base = base if base is not None else RandomForestClassifier(n_estimators=300, max_depth=12, random_state=RANDOM_STATE, class_weight=\"balanced\")\n",
    "        self.model = CalibratedClassifierCV(estimator=clone(base), method=\"sigmoid\", cv=3)\n",
    "    def fit(self, X, y, treatment):\n",
    "        X = np.asarray(X); t = np.asarray(treatment).ravel(); y = np.asarray(y).ravel()\n",
    "        self.model.fit(np.column_stack([X, t]), y)\n",
    "        return self\n",
    "    def _proba_aug(self, X, a):\n",
    "        return self.model.predict_proba(np.column_stack([np.asarray(X), np.asarray(a).ravel()]))[:,1]\n",
    "    def mu0(self, X): return self._proba_aug(X, np.zeros(len(X)))\n",
    "    def mu1(self, X): return self._proba_aug(X, np.ones(len(X)))\n",
    "\n",
    "class TLearner(_UpliftBase, BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, base=None):\n",
    "        self.base = base if base is not None else RandomForestClassifier(n_estimators=300, max_depth=12, random_state=RANDOM_STATE, class_weight=\"balanced\")\n",
    "    def fit(self, X, y, treatment):\n",
    "        X = np.asarray(X); y = np.asarray(y).ravel(); t = np.asarray(treatment).ravel()\n",
    "        self.m0 = clone(self.base); self.m1 = clone(self.base)\n",
    "        m1 = t==1; m0 = ~m1\n",
    "        self.m0.fit(X[m0], y[m0]) if m0.sum()>0 else self.m0.fit(X, y)\n",
    "        self.m1.fit(X[m1], y[m1]) if m1.sum()>0 else self.m1.fit(X, y)\n",
    "        return self\n",
    "    def mu0(self, X): return self.m0.predict_proba(X)[:,1]\n",
    "    def mu1(self, X): return self.m1.predict_proba(X)[:,1]\n",
    "\n",
    "class XLearner(_UpliftBase, BaseEstimator):\n",
    "    def __init__(self, base_cls=None, base_reg=None, prop_model=None):\n",
    "        self.base_cls = base_cls if base_cls is not None else RandomForestClassifier(n_estimators=300, max_depth=12, random_state=RANDOM_STATE, class_weight=\"balanced\")\n",
    "        self.base_reg = base_reg if base_reg is not None else RandomForestRegressor(n_estimators=300, max_depth=12, random_state=RANDOM_STATE)\n",
    "        self.prop_model = prop_model if prop_model is not None else LogisticRegression(max_iter=1000, random_state=RANDOM_STATE)\n",
    "    def fit(self, X, y, treatment):\n",
    "        X = np.asarray(X); y = np.asarray(y).ravel(); t = np.asarray(treatment).ravel()\n",
    "        self.m0 = clone(self.base_cls); self.m1 = clone(self.base_cls)\n",
    "        m1 = t==1; m0 = ~m1\n",
    "        self.m0.fit(X[m0], y[m0]) if m0.sum()>0 else self.m0.fit(X, y)\n",
    "        self.m1.fit(X[m1], y[m1]) if m1.sum()>0 else self.m1.fit(X, y)\n",
    "        mu0_t = self.m0.predict_proba(X[m1])[:,1] if m1.sum()>0 else np.full(m1.sum(), y.mean())\n",
    "        mu1_c = self.m1.predict_proba(X[m0])[:,1] if m0.sum()>0 else np.full(m0.sum(), y.mean())\n",
    "        D1 = y[m1] - mu0_t\n",
    "        D0 = mu1_c - y[m0]\n",
    "        self.tau1 = clone(self.base_reg).fit(X[m1], D1) if m1.sum()>0 else clone(self.base_reg).fit(X, np.zeros(len(X)))\n",
    "        self.tau0 = clone(self.base_reg).fit(X[m0], D0) if m0.sum()>0 else clone(self.base_reg).fit(X, np.zeros(len(X)))\n",
    "        self.g = clone(self.prop_model).fit(X, t)\n",
    "        return self\n",
    "    def mu0(self, X): return self.m0.predict_proba(X)[:,1]\n",
    "    def mu1(self, X): return self.m1.predict_proba(X)[:,1]\n",
    "    def uplift(self, X):\n",
    "        e = self.g.predict_proba(X)[:,1]\n",
    "        return (1 - e) * self.tau1.predict(X) + e * self.tau0.predict(X)\n",
    "\n",
    "class DRLearner(_UpliftBase, BaseEstimator):\n",
    "    def __init__(self, base_cls=None, base_reg=None, prop_model=None):\n",
    "        self.base_cls = base_cls if base_cls is not None else RandomForestClassifier(n_estimators=300, max_depth=12, random_state=RANDOM_STATE, class_weight=\"balanced\")\n",
    "        self.base_reg = base_reg if base_reg is not None else RandomForestRegressor(n_estimators=300, max_depth=12, random_state=RANDOM_STATE)\n",
    "        self.prop_model = prop_model if prop_model is not None else LogisticRegression(max_iter=1000, random_state=RANDOM_STATE)\n",
    "    def fit(self, X, y, treatment):\n",
    "        X = np.asarray(X); y = np.asarray(y).ravel(); t = np.asarray(treatment).ravel()\n",
    "        self.m0 = clone(self.base_cls); self.m1 = clone(self.base_cls)\n",
    "        m1 = t==1; m0 = ~m1\n",
    "        self.m0.fit(X[m0], y[m0]) if m0.sum()>0 else self.m0.fit(X, y)\n",
    "        self.m1.fit(X[m1], y[m1]) if m1.sum()>0 else self.m1.fit(X, y)\n",
    "        self.g = LogisticRegression(max_iter=1000, random_state=RANDOM_STATE).fit(X, t)\n",
    "        e = np.clip(self.g.predict_proba(X)[:,1], 1e-3, 1-1e-3)\n",
    "        mu0 = self.m0.predict_proba(X)[:,1]; mu1 = self.m1.predict_proba(X)[:,1]\n",
    "        mu_t = np.where(m1, mu1, mu0)\n",
    "        phi = ((t - e) / (e * (1 - e))) * (y - mu_t) + (mu1 - mu0)\n",
    "        self.tau = clone(self.base_reg).fit(X, phi)\n",
    "        return self\n",
    "    def mu0(self, X): return self.m0.predict_proba(X)[:,1]\n",
    "    def mu1(self, X): return self.m1.predict_proba(X)[:,1]\n",
    "    def uplift(self, X): return self.tau.predict(X)\n",
    "\n",
    "class RLearner(_UpliftBase, BaseEstimator):\n",
    "    def __init__(self, base_cls=None, base_reg=None):\n",
    "        self.base_cls = base_cls if base_cls is not None else RandomForestClassifier(n_estimators=300, max_depth=12, random_state=RANDOM_STATE, class_weight=\"balanced\")\n",
    "        self.base_reg = base_reg if base_reg is not None else RandomForestRegressor(n_estimators=300, max_depth=12, random_state=RANDOM_STATE)\n",
    "        self.mu_model = CalibratedClassifierCV(estimator=clone(self.base_cls), method=\"sigmoid\", cv=3)\n",
    "    def fit(self, X, y, treatment):\n",
    "        X = np.asarray(X); y = np.asarray(y).ravel(); t = np.asarray(treatment).ravel()\n",
    "        self.mu = self.mu_model.fit(X, y)\n",
    "        self.g = CalibratedClassifierCV(estimator=LogisticRegression(max_iter=1000, random_state=RANDOM_STATE), method=\"sigmoid\", cv=3).fit(X, t)\n",
    "        mu_hat = self.mu.predict_proba(X)[:,1]\n",
    "        e = np.clip(self.g.predict_proba(X)[:,1], 1e-3, 1-1e-3)\n",
    "        y_res, t_res = y - mu_hat, t - e\n",
    "        target = y_res / (t_res + np.sign(t_res)*EPS)\n",
    "        weights = np.abs(t_res)\n",
    "        self.tau = clone(self.base_reg).fit(X, target, sample_weight=weights)\n",
    "        return self\n",
    "    def mu0(self, X):\n",
    "        mu = self.mu.predict_proba(X)[:,1]; e = np.clip(self.g.predict_proba(X)[:,1], 1e-3, 1-1e-3)\n",
    "        tau = self.tau.predict(X)\n",
    "        return np.clip(mu - e * tau, 0.0, 1.0)\n",
    "    def mu1(self, X):\n",
    "        mu = self.mu.predict_proba(X)[:,1]; e = np.clip(self.g.predict_proba(X)[:,1], 1e-3, 1-1e-3)\n",
    "        tau = self.tau.predict(X)\n",
    "        return np.clip(mu + (1 - e) * tau, 0.0, 1.0)\n",
    "\n",
    "class DRForest(_UpliftBase, BaseEstimator):\n",
    "    def __init__(self, base_cls=None):\n",
    "        self.base_cls = base_cls if base_cls is not None else RandomForestClassifier(n_estimators=300, max_depth=12, random_state=RANDOM_STATE, class_weight=\"balanced\")\n",
    "        self.m0 = None; self.m1 = None; self.g = None; self.reg = RandomForestRegressor(n_estimators=400, max_depth=14, random_state=RANDOM_STATE)\n",
    "    def fit(self, X, y, treatment):\n",
    "        X = np.asarray(X); y = np.asarray(y).ravel(); t = np.asarray(treatment).ravel()\n",
    "        self.m0 = clone(self.base_cls); self.m1 = clone(self.base_cls)\n",
    "        m1 = t==1; m0 = ~m1\n",
    "        self.m0.fit(X[m0], y[m0]) if m0.sum()>0 else self.m0.fit(X, y)\n",
    "        self.m1.fit(X[m1], y[m1]) if m1.sum()>0 else self.m1.fit(X, y)\n",
    "        self.g = CalibratedClassifierCV(estimator=LogisticRegression(max_iter=1000, random_state=RANDOM_STATE), method=\"sigmoid\", cv=3).fit(X, t)\n",
    "        e = np.clip(self.g.predict_proba(X)[:,1], 1e-3, 1-1e-3)\n",
    "        mu0 = self.m0.predict_proba(X)[:,1]; mu1 = self.m1.predict_proba(X)[:,1]\n",
    "        mu_t = np.where(m1, mu1, mu0)\n",
    "        phi = ((t - e) / (e * (1 - e))) * (y - mu_t) + (mu1 - mu0)\n",
    "        self.tau = clone(self.reg).fit(X, phi)\n",
    "        return self\n",
    "    def mu0(self, X): return self.m0.predict_proba(X)[:,1]\n",
    "    def mu1(self, X): return self.m1.predict_proba(X)[:,1]\n",
    "    def uplift(self, X): return self.tau.predict(X)\n",
    "\n",
    "class DRExtremeTrees(DRForest):\n",
    "    def __init__(self, base_cls=None):\n",
    "        super().__init__(base_cls=base_cls)\n",
    "        self.reg = ExtraTreesRegressor(n_estimators=600, max_depth=16, random_state=RANDOM_STATE)\n",
    "\n",
    "class DRXGBoost(DRForest):\n",
    "    def __init__(self, base_cls=None):\n",
    "        if not XGBOOST_AVAILABLE:\n",
    "            raise ImportError(\"xgboost not available\")\n",
    "        super().__init__(base_cls=base_cls)\n",
    "        self.reg = xgb.XGBRegressor(n_estimators=600, max_depth=6, learning_rate=0.05, subsample=0.8, colsample_bytree=0.8, random_state=RANDOM_STATE, objective=\"reg:squarederror\")\n",
    "\n",
    "# =============================================================\n",
    "# Model zoo\n",
    "# =============================================================\n",
    "\n",
    "def build_models():\n",
    "    base_cls = RandomForestClassifier(n_estimators=300, max_depth=12, random_state=RANDOM_STATE, class_weight=\"balanced\")\n",
    "    base_reg = RandomForestRegressor(n_estimators=400, max_depth=14, random_state=RANDOM_STATE)\n",
    "    models = {\n",
    "        \"S-Learner\": SLearner(base=base_cls),\n",
    "        \"T-Learner\": TLearner(base=base_cls),\n",
    "        \"X-Learner\": XLearner(base_cls=base_cls, base_reg=base_reg),\n",
    "        \"DR-Learner\": DRLearner(base_cls=base_cls, base_reg=base_reg),\n",
    "        \"R-Learner\": RLearner(base_cls=base_cls, base_reg=base_reg),\n",
    "        \"CausalForest\": DRForest(base_cls=base_cls),\n",
    "        \"CausalExtraTrees\": DRExtremeTrees(base_cls=base_cls),\n",
    "    }\n",
    "    if XGBOOST_AVAILABLE:\n",
    "        try:\n",
    "            models[\"CausalXGBoost\"] = DRXGBoost(base_cls=base_cls)\n",
    "        except Exception:\n",
    "            pass\n",
    "    return models\n",
    "\n",
    "# =============================================================\n",
    "# OPE helpers & diagnostics\n",
    "# =============================================================\n",
    "\n",
    "def estimate_propensity(X, t):\n",
    "    t = np.asarray(t).ravel()\n",
    "    p_hat = float(np.mean(t))\n",
    "    if np.unique(t).size < 2:\n",
    "        print(f\"  [propensity] Single-class in TRAIN (mean={p_hat:.3f}). Using constant propensity.\")\n",
    "        class _ConstP:\n",
    "            def predict_proba(self, X_):\n",
    "                n = len(X_)\n",
    "                e = np.full((n, 1), p_hat, dtype=float)\n",
    "                return np.hstack([1 - e, e])\n",
    "        return _ConstP()\n",
    "    lr = LogisticRegression(max_iter=2000, random_state=RANDOM_STATE)\n",
    "    cal = CalibratedClassifierCV(estimator=lr, method=\"sigmoid\", cv=3)\n",
    "    cal.fit(X, t)\n",
    "    return cal\n",
    "\n",
    "\n",
    "def _clip_weights(w: np.ndarray):\n",
    "    thr = np.percentile(w, 99)\n",
    "    return np.minimum(w, thr)\n",
    "\n",
    "\n",
    "def dr_policy_risk(y, t, mu0, mu1, e, policy_t):\n",
    "    mu_pi = np.where(policy_t==1, mu1, mu0)\n",
    "    mu_t_obs = np.where(t==1, mu1, mu0)\n",
    "    w = np.where(t==1, 1/np.clip(e, 1e-6, 1-1e-6), 1/np.clip(1-e, 1e-6, 1-1e-6))\n",
    "    w = _clip_weights(w)\n",
    "    adj = (t==policy_t) * (y - mu_t_obs) * w\n",
    "    return float(np.mean(mu_pi + adj))\n",
    "\n",
    "\n",
    "def ips_policy_risk(y, t, e, policy_t):\n",
    "    w = ((policy_t==1) * (t==1) / np.clip(e, 1e-6, 1-1e-6)) + ((policy_t==0) * (t==0) / np.clip(1-e, 1e-6, 1-1e-6))\n",
    "    w = _clip_weights(w)\n",
    "    risk = float(np.sum(w * y) / np.sum(w))\n",
    "    ess = float((np.sum(w)**2) / np.sum(w**2))\n",
    "    wstats = {\"w_mean\": float(np.mean(w)), \"w_max\": float(np.max(w)), \"w_min\": float(np.min(w)), \"w_p99\": float(np.percentile(w, 99))}\n",
    "    return risk, ess, wstats\n",
    "\n",
    "\n",
    "def arraysafe(a):\n",
    "    return np.array(list(a), dtype=float)\n",
    "\n",
    "\n",
    "def capacity_curve(y, t, mu0, mu1, e, fractions):\n",
    "    \"\"\"MODIFIED to return ARR vs. Watchful Waiting and ARR vs. Status Quo.\"\"\"\n",
    "    policy0 = np.zeros_like(y)\n",
    "    risk_none_dr = dr_policy_risk(y, t, mu0, mu1, e, policy0)\n",
    "    status_quo_risk = np.mean(y)\n",
    "\n",
    "    benefit = mu0 - mu1\n",
    "    order = np.argsort(-benefit)\n",
    "\n",
    "    arr_dr, arr_sq = [], []\n",
    "    for f in fractions:\n",
    "        k = int(round(f * len(y)))\n",
    "        policy = np.zeros_like(y)\n",
    "        if k > 0:\n",
    "            policy[order[:k]] = 1\n",
    "        risk_at_f = dr_policy_risk(y, t, mu0, mu1, e, policy)\n",
    "        arr_dr.append(risk_none_dr - risk_at_f)\n",
    "        arr_sq.append(status_quo_risk - risk_at_f)\n",
    "    return arraysafe(fractions), np.array(arr_dr), np.array(arr_sq)\n",
    "\n",
    "\n",
    "def qini_auuc_dr(y, t, mu0, mu1, e):\n",
    "    mu_t = np.where(t==1, mu1, mu0)\n",
    "    tau_dr = (mu0 - mu1) + (t/np.clip(e,1e-6,1-1e-6) - (1-t)/np.clip(1-e,1e-6,1-1e-6)) * (y - mu_t)\n",
    "    benefit_pred = mu0 - mu1\n",
    "    order = np.argsort(-benefit_pred)\n",
    "    gain_cum = np.cumsum(tau_dr[order])\n",
    "    frac = np.arange(1, len(y)+1) / len(y)\n",
    "    uplift_curve = gain_cum / np.arange(1, len(y)+1)\n",
    "    auuc = float(np.trapz(uplift_curve, x=frac))\n",
    "    return frac, uplift_curve, auuc\n",
    "\n",
    "\n",
    "def smd_by_feature(X, t, keep_mask=None, top_k=20):\n",
    "    df = pd.DataFrame(X)\n",
    "    cols = list(range(df.shape[1]))\n",
    "    out = []\n",
    "    for stage, mask in [(\"pre\", np.ones(len(t), dtype=bool)), (\"post\", keep_mask if keep_mask is not None else np.ones(len(t), dtype=bool))]:\n",
    "        mt = t[mask]==1; mc = t[mask]==0\n",
    "        for j,c in enumerate(cols[:top_k]):\n",
    "            x_t = df.loc[mask, c].values[mt]\n",
    "            x_c = df.loc[mask, c].values[mc]\n",
    "            if len(x_t)==0 or len(x_c)==0: continue\n",
    "            d = (np.mean(x_t) - np.mean(x_c)) / (np.sqrt((np.var(x_t, ddof=1)+np.var(x_c, ddof=1))/2)+1e-9)\n",
    "            out.append({\"feature\": f\"f{c}\", \"stage\": stage, \"SMD\": float(d)})\n",
    "    return pd.DataFrame(out)\n",
    "\n",
    "# =============================================================\n",
    "# Publication outputs\n",
    "# =============================================================\n",
    "\n",
    "def save_table3(metrics_by_model: dict):\n",
    "    rows = []\n",
    "    for name, m in metrics_by_model.items():\n",
    "        nnt_vs_ww = (1.0/m['arr_dr_full']) if m['arr_dr_full'] > 0 else \"N/A\"\n",
    "        if isinstance(nnt_vs_ww, float): nnt_vs_ww = f\"{nnt_vs_ww:.1f}\"\n",
    "        nnt_vs_sq = (1.0/m['arr_vs_status_quo']) if m['arr_vs_status_quo'] > 0 else \"N/A\"\n",
    "        if isinstance(nnt_vs_sq, float): nnt_vs_sq = f\"{nnt_vs_sq:.1f}\"\n",
    "        row = {\n",
    "            \"Model\": name,\n",
    "            \"ARR_vs_WatchfulWaiting\": f\"{m['arr_dr_full']:.4f}\",\n",
    "            \"NNT_vs_WatchfulWaiting\": nnt_vs_ww,\n",
    "            \"ARR_vs_StatusQuo\": f\"{m['arr_vs_status_quo']:.4f}\",\n",
    "            \"NNT_vs_StatusQuo\": nnt_vs_sq,\n",
    "            \"AUUC_DR\": f\"{m['auuc_dr']:.4f}\",\n",
    "            \"TreatRate@full\": f\"{m['treat_rate']:.3f}\",\n",
    "            \"ESS@full\": f\"{m['ess']:.1f}\",\n",
    "        }\n",
    "        rows.append(row)\n",
    "    pd.DataFrame(rows).to_csv(OUTPUT_DIR/\"table3_intervention_ope.csv\", index=False)\n",
    "\n",
    "\n",
    "def save_figures(best_name: str, cap_frac, cap_arr_dr, cap_arr_sq, q_frac, q_curve):\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.plot(cap_frac, cap_arr_dr, label=\"ARR vs. Watchful Waiting\", linewidth=2)\n",
    "    plt.plot(cap_frac, cap_arr_sq, label=\"ARR vs. Status Quo\", linewidth=2, linestyle='--')\n",
    "    plt.axhline(0, color='black', linewidth=0.5, linestyle='-')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.xlabel(\"Fraction Treated (capacity)\")\n",
    "    plt.ylabel(\"Absolute Risk Reduction (ARR)\")\n",
    "    plt.title(f\"Figure 2A: Capacity Curve â€” {best_name}\")\n",
    "    plt.legend(); plt.tight_layout()\n",
    "    plt.savefig(OUTPUT_DIR/\"figure2_capacity_curve.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.plot(q_frac, q_curve, linewidth=2)\n",
    "    plt.xlabel(\"Fraction of population (ranked by predicted benefit)\")\n",
    "    plt.ylabel(\"Cumulative mean benefit (DR)\")\n",
    "    plt.title(f\"Figure 2B: Qini / Uplift Curve â€” {best_name}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUTPUT_DIR/\"figure2_qini_uplift.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "# =============================================================\n",
    "# Bootstrap capacity CIs\n",
    "# =============================================================\n",
    "\n",
    "def bootstrap_capacity(y, t, mu0, mu1, e, fractions, B=200, seed=RANDOM_STATE):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    n = len(y)\n",
    "    arr_mat = np.zeros((B, len(fractions)))\n",
    "    arr_full = np.zeros(B)\n",
    "    auuc_b = np.zeros(B)\n",
    "    for b in range(B):\n",
    "        idx = rng.integers(0, n, n)\n",
    "        fb, arr_dr_b, _ = capacity_curve(y[idx], t[idx], mu0[idx], mu1[idx], e[idx], fractions)\n",
    "        arr_mat[b,:] = arr_dr_b\n",
    "        policy_full = (mu1[idx] < mu0[idx]).astype(int)\n",
    "        r0 = dr_policy_risk(y[idx], t[idx], mu0[idx], mu1[idx], e[idx], np.zeros_like(y[idx]))\n",
    "        r1 = dr_policy_risk(y[idx], t[idx], mu0[idx], mu1[idx], e[idx], policy_full)\n",
    "        arr_full[b] = r0 - r1\n",
    "        _, _, au = qini_auuc_dr(y[idx], t[idx], mu0[idx], mu1[idx], e[idx])\n",
    "        auuc_b[b] = au\n",
    "    lo = np.percentile(arr_mat, 2.5, axis=0); hi = np.percentile(arr_mat, 97.5, axis=0)\n",
    "    arr_full_ci = (float(np.percentile(arr_full, 2.5)), float(np.percentile(arr_full, 97.5)))\n",
    "    auuc_ci = (float(np.percentile(auuc_b, 2.5)), float(np.percentile(auuc_b, 97.5)))\n",
    "    return {\n",
    "        \"arr_curve_lo\": lo, \"arr_curve_hi\": hi,\n",
    "        \"arr_full_ci\": arr_full_ci, \"auuc_ci\": auuc_ci,\n",
    "    }\n",
    "\n",
    "# =============================================================\n",
    "# Runner\n",
    "# =============================================================\n",
    "\n",
    "def run():\n",
    "    print(\"[Block 3] Loading inputsâ€¦\")\n",
    "    features = _load_features()\n",
    "    interventions = _load_interventions()\n",
    "\n",
    "    print(\"Preparing engagedâ€‘only train/test and treatmentsâ€¦\")\n",
    "    X_tr, X_te, y_tr, y_te, t_tr, t_te, te_ids, meta = prepare_intervention_frame(features, interventions)\n",
    "\n",
    "    print(\"Building modelsâ€¦\")\n",
    "    models = build_models()\n",
    "\n",
    "    print(\"Fitting behavior policy (propensity) on TRAINâ€¦\")\n",
    "    prop = estimate_propensity(X_tr, t_tr)\n",
    "    e_te = np.clip(prop.predict_proba(X_te)[:,1], 1e-6, 1-1e-6)\n",
    "\n",
    "    def do_trim(e, lo, hi):\n",
    "        return (e >= lo) & (e <= hi)\n",
    "\n",
    "    lo_hi_candidates = [(0.05,0.95), (0.02,0.98), (0.01,0.99)]\n",
    "    keep = None\n",
    "    for lo,hi in lo_hi_candidates:\n",
    "        keep = do_trim(e_te, lo, hi)\n",
    "        if keep.sum() >= max(50, int(0.05*len(e_te))):\n",
    "            print(f\"  âœ‚ï¸  Overlap trim: kept {keep.sum()} / {len(e_te)} (e in [{lo:.2f}, {hi:.2f}])\")\n",
    "            break\n",
    "    if keep is None or keep.sum()==0:\n",
    "        print(\"  âœ‚ï¸  Overlap trim skipped (no samples). Using untrimmed test set.\")\n",
    "        keep = np.ones_like(e_te, dtype=bool)\n",
    "    X_te_k, y_te_k, t_te_k, e_te_k = X_te[keep], y_te[keep], t_te[keep], e_te[keep]\n",
    "    te_ids_k = te_ids[keep]\n",
    "\n",
    "    smd_df = smd_by_feature(X_te, t_te, keep_mask=keep, top_k=20)\n",
    "    smd_df.to_csv(OUTPUT_DIR/\"balance_table.csv\", index=False)\n",
    "\n",
    "    print(\"Fitting & evaluating modelsâ€¦\")\n",
    "    metrics_by_model, mu_cache = {}, {}\n",
    "\n",
    "    fractions = np.unique(np.concatenate([np.linspace(0, 1, 21), np.linspace(0, 0.5, 11)]))\n",
    "\n",
    "    for name, model in models.items():\n",
    "        try:\n",
    "            model.fit(X_tr, y_tr, treatment=t_tr)\n",
    "            mu0 = model.mu0(X_te_k); mu1 = model.mu1(X_te_k)\n",
    "            mu_cache[name] = {\"mu0\": mu0.tolist(), \"mu1\": mu1.tolist()}\n",
    "\n",
    "            policy_full = (mu1 < mu0).astype(int)\n",
    "            risk_none_dr = dr_policy_risk(y_te_k, t_te_k, mu0, mu1, e_te_k, np.zeros_like(y_te_k))\n",
    "            risk_full_dr = dr_policy_risk(y_te_k, t_te_k, mu0, mu1, e_te_k, policy_full)\n",
    "            _, ess, wstats = ips_policy_risk(y_te_k, t_te_k, e_te_k, policy_full)\n",
    "\n",
    "            arr_dr_full = risk_none_dr - risk_full_dr\n",
    "            status_quo_risk = np.mean(y_te_k)\n",
    "            arr_vs_status_quo = status_quo_risk - risk_full_dr\n",
    "\n",
    "            cap_frac, cap_arr_dr, cap_arr_sq = capacity_curve(y_te_k, t_te_k, mu0, mu1, e_te_k, fractions)\n",
    "            q_frac, q_curve, auuc = qini_auuc_dr(y_te_k, t_te_k, mu0, mu1, e_te_k)\n",
    "            \n",
    "            metrics_by_model[name] = {\n",
    "                \"treat_rate\": float(policy_full.mean()), \"arr_dr_full\": float(arr_dr_full),\n",
    "                \"arr_vs_status_quo\": float(arr_vs_status_quo), \"ess\": ess, \"wstats\": wstats, \"auuc_dr\": auuc,\n",
    "                \"cap_frac\": cap_frac.tolist(), \"cap_arr_dr\": cap_arr_dr.tolist(), \"cap_arr_sq\": cap_arr_sq.tolist(),\n",
    "                \"q_frac\": q_frac.tolist(), \"q_curve\": q_curve.tolist(),\n",
    "            }\n",
    "            print(f\"  {name}: ARR_vs_WW={arr_dr_full:.4f} | ARR_vs_SQ={arr_vs_status_quo:.4f} | AUUC={auuc:.4f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  {name}: FAILED â€” {e}\")\n",
    "\n",
    "    if not metrics_by_model:\n",
    "        raise RuntimeError(\"All intervention models failed. Check inputs.\")\n",
    "\n",
    "    best = 'CausalForest'\n",
    "    if best not in metrics_by_model:\n",
    "        print(f\"[Warning] '{best}' not found in results. Falling back to best AUUC model.\")\n",
    "        best = max(metrics_by_model, key=lambda k: metrics_by_model[k]['auuc_dr'])\n",
    "    \n",
    "    print(f\"\\nSelected '{best}' as the best model for reporting and plotting.\")\n",
    "    m = metrics_by_model[best]\n",
    "\n",
    "    save_table3(metrics_by_model)\n",
    "    save_figures(best, np.array(m['cap_frac']), np.array(m['cap_arr_dr']), np.array(m['cap_arr_sq']), np.array(m['q_frac']), np.array(m['q_curve']))\n",
    "\n",
    "    cap_df = pd.DataFrame({\n",
    "        \"fraction\": m['cap_frac'], \"ARR_DR\": m['cap_arr_dr'], \"ARR_vs_SQ\": m['cap_arr_sq'],\n",
    "    })\n",
    "    cap_df.to_csv(OUTPUT_DIR/\"capacity_curve_best.csv\", index=False)\n",
    "\n",
    "    # ... [rest of the saving logic remains the same] ...\n",
    "    sens_fracs = [0.10, 0.15, 0.20, 0.25]\n",
    "    sens = []\n",
    "    for f in sens_fracs:\n",
    "        idx = int(np.argmin(np.abs(np.array(m['cap_frac']) - f)))\n",
    "        arr_val = float(m['cap_arr_dr'][idx])\n",
    "        sens.append({\n",
    "            \"fraction\": float(m['cap_frac'][idx]), \"ARR_DR\": arr_val,\n",
    "            \"NNT\": (1.0/arr_val) if arr_val > 0 else np.nan,\n",
    "        })\n",
    "    pd.DataFrame(sens).to_csv(OUTPUT_DIR/\"capacity_sensitivity.csv\", index=False)\n",
    "\n",
    "    mu0_b = np.array(mu_cache[best]['mu0']); mu1_b = np.array(mu_cache[best]['mu1'])\n",
    "    policy_b = (mu1_b < mu0_b).astype(int)\n",
    "    rec = np.where(policy_b==1, \"any_intervention\", \"watchful_waiting\")\n",
    "    out_pred = pd.DataFrame({\n",
    "        \"patient_id\": te_ids_k, \"recommended_intervention\": rec,\n",
    "        \"mu0\": mu0_b, \"mu1\": mu1_b, \"benefit\": (mu0_b - mu1_b),\n",
    "        \"policy_treat\": policy_b, \"model_name\": best, \"horizon_days\": HORIZON_DAYS,\n",
    "    })\n",
    "    out_pred.to_csv(OUTPUT_DIR/\"best_intervention_model_predictions.csv\", index=False)\n",
    "\n",
    "    best_model = build_models()[best]\n",
    "    best_model.fit(X_tr, y_tr, treatment=t_tr)\n",
    "    mu0_full = best_model.mu0(X_te); mu1_full = best_model.mu1(X_te)\n",
    "    policy_full = (mu1_full < mu0_full).astype(int)\n",
    "    rec_full = np.where(policy_full==1, \"any_intervention\", \"watchful_waiting\")\n",
    "    out_pred_full = pd.DataFrame({\n",
    "        \"patient_id\": te_ids, \"recommended_intervention\": rec_full,\n",
    "        \"mu0\": mu0_full, \"mu1\": mu1_full, \"benefit\": (mu0_full - mu1_full),\n",
    "        \"policy_treat\": policy_full, \"model_name\": best, \"horizon_days\": HORIZON_DAYS,\n",
    "    })\n",
    "    out_pred_full.to_csv(OUTPUT_DIR/\"best_intervention_model_predictions_fulltest.csv\", index=False)\n",
    "\n",
    "    Path(OUTPUT_DIR/\"intervention_ope_diagnostics.json\").write_text(json.dumps(metrics_by_model, indent=2))\n",
    "\n",
    "    print(\"\\nâœ“ Block 3 complete. Wrote:\")\n",
    "    print(\"  â€¢ table3_intervention_ope.csv\")\n",
    "    print(\"  â€¢ figure2_capacity_curve.png\")\n",
    "    print(\"  â€¢ figure2_qini_uplift.png\")\n",
    "    print(\"  â€¢ capacity_curve_best.csv\")\n",
    "    print(\"  â€¢ capacity_sensitivity.csv\")\n",
    "    print(\"  â€¢ balance_table.csv\")\n",
    "    print(\"  â€¢ best_intervention_model_predictions.csv\")\n",
    "    print(\"  â€¢ best_intervention_model_predictions_fulltest.csv\")\n",
    "    print(\"  â€¢ intervention_ope_diagnostics.json\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d558a587-2a61-488f-b4b4-60027782284b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xlsxwriter\n",
    "# or (fallback engine)\n",
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ce2320-4e7c-4e24-834f-9f0e4457c944",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "BLOCK 4 â€” CLINICAL VALIDATION PACK (engaged-only v2)\n",
    "===================================================\n",
    "Builds a 200-case review pack **from engaged patients only**, collapses\n",
    "encounter notes per patient (only encounters where `encounterOccurred == 'YES'`),\n",
    "restores signal-risk summaries, adds diagnosis summary (DX_Summary) from feature\n",
    "columns, and writes a tidy Excel workbook with **two columns per reviewer**\n",
    "(Risk & Action) plus free-text notes. Also writes a CSV mirror and a\n",
    "Case_ID â†” patient_id mapping.\n",
    "\n",
    "Inputs (from ./publication_outputs):\n",
    "  â€¢ feature_matrix.parquet\n",
    "  â€¢ interventions_df.csv           (free-text notes, optional columns)\n",
    "  â€¢ signal_risks.csv               (optional; per-patient signals)\n",
    "  â€¢ feature_meta.json              (for feature list)\n",
    "  â€¢ best_tte_model_predictions_30d.csv (optional; for risk quartiles)\n",
    "\n",
    "Outputs:\n",
    "  â€¢ clinical_validation_200_cases.xlsx\n",
    "  â€¢ clinical_validation_200_cases.csv\n",
    "  â€¢ clinical_validation_case_index_mapping.csv (Case_ID â†” patient_id)\n",
    "\n",
    "Reviewer sheets include data validation drop-downs for: Risk level, Next Action.\n",
    "Notes columns are wrapped and sized. Text is contained within cells (no bleed).\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "import json\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    from openpyxl import load_workbook\n",
    "    from openpyxl.utils import get_column_letter\n",
    "    from openpyxl.worksheet.datavalidation import DataValidation\n",
    "except Exception:\n",
    "    load_workbook = None\n",
    "\n",
    "OUTPUT_DIR = Path(\"publication_outputs\"); OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "RANDOM_STATE = 42\n",
    "N_CASES = 200\n",
    "HIGH_RISK_RATIO = 0.75  # target share of cases from the highest risk quartile\n",
    "HORIZON_DAYS = 30\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Resilient loaders\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def _load_features() -> pd.DataFrame:\n",
    "    for p in [OUTPUT_DIR/\"feature_matrix.parquet\", OUTPUT_DIR/\"features_df.parquet\"]:\n",
    "        if p.exists():\n",
    "            print(f\"Found features: {p}\")\n",
    "            return pd.read_parquet(p)\n",
    "    raise FileNotFoundError(\"feature_matrix.parquet not found in publication_outputs.\")\n",
    "\n",
    "\n",
    "def _load_interventions() -> pd.DataFrame:\n",
    "    for p in [OUTPUT_DIR/\"interventions_df.csv\", OUTPUT_DIR/\"interventions_processed.csv\"]:\n",
    "        if p.exists():\n",
    "            print(f\"Found interventions: {p}\")\n",
    "            return pd.read_csv(p)\n",
    "    print(\"âš ï¸ No interventions file found â€” proceeding without note joins.\")\n",
    "    return pd.DataFrame()\n",
    "\n",
    "\n",
    "def _load_signal_risks() -> pd.DataFrame | None:\n",
    "    p = OUTPUT_DIR/\"signal_risks.csv\"\n",
    "    if p.exists():\n",
    "        print(f\"Found signal_risks: {p}\")\n",
    "        return pd.read_csv(p)\n",
    "    return None\n",
    "\n",
    "\n",
    "def _load_pred_30d() -> pd.DataFrame | None:\n",
    "    for p in [OUTPUT_DIR/\"best_tte_model_predictions_30d.csv\", OUTPUT_DIR/\"best_tte_model_predictions.csv\"]:\n",
    "        if Path(p).exists():\n",
    "            return pd.read_csv(p)\n",
    "    return None\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Helpers for interventions & notes\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def _normalize_interventions_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    if \"patient_id\" not in df.columns and \"patientId\" in df.columns:\n",
    "        df = df.rename(columns={\"patientId\": \"patient_id\"})\n",
    "    return df\n",
    "\n",
    "\n",
    "def _pick_note_column(df: pd.DataFrame) -> str | None:\n",
    "    if \"Encounter_Notes\" in df.columns:\n",
    "        return \"Encounter_Notes\"\n",
    "    def norm(c): return str(c).lower().replace(\"_\", \"\").replace(\" \", \"\")\n",
    "    name_map = {c: norm(c) for c in df.columns}\n",
    "    ranked = [c for c,nc in name_map.items() if (\"encounter\" in nc and \"note\" in nc)]\n",
    "    for c,nc in name_map.items():\n",
    "        if nc in {\"notes\",\"note\",\"text\",\"content\",\"body\"}:\n",
    "            ranked.append(c)\n",
    "    return ranked[0] if ranked else None\n",
    "\n",
    "\n",
    "def _clean_note_text(x: str) -> str:\n",
    "    if not isinstance(x, str):\n",
    "        x = str(x)\n",
    "    # normalize line breaks, remove stray control chars except tab/newline\n",
    "    t = x.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "    t = re.sub(r\"[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f]\", \" \", t)\n",
    "    # collapse massive whitespace runs\n",
    "    t = re.sub(r\"\\s{3,}\", \"  \", t)\n",
    "    return t.strip()\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Diagnosis summary from feature columns (dx_cat_* counts/flags)\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def build_dx_summary(features: pd.DataFrame) -> pd.Series:\n",
    "    # Look for columns like dx_cat_* (numeric)\n",
    "    mask = [c for c in features.columns if c.lower().startswith(\"dx_cat_\")]\n",
    "    if not mask:\n",
    "        return pd.Series(index=features.index, dtype=object)\n",
    "    dx_df = features[mask].copy()\n",
    "    # keep only numeric\n",
    "    dx_df = dx_df.loc[:, [c for c in dx_df.columns if pd.api.types.is_numeric_dtype(dx_df[c])]]\n",
    "    cats = [re.sub(r\"^dx_cat_\", \"\", c, flags=re.I) for c in dx_df.columns]\n",
    "    dx_df.columns = cats\n",
    "\n",
    "    def summarize(row):\n",
    "        pos = [(c, float(row[c])) for c in dx_df.columns if pd.notnull(row[c]) and float(row[c]) > 0]\n",
    "        if not pos:\n",
    "            return \"\"\n",
    "        # sort by count/flag descending, then alphabetically for ties\n",
    "        pos.sort(key=lambda x: (-x[1], x[0]))\n",
    "        top = [c for c,_ in pos[:3]]\n",
    "        return \"; \".join(top)\n",
    "\n",
    "    return dx_df.apply(summarize, axis=1)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Risk quartiles from predictions (preferred) or fallback to outcome_30d\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def label_risk_quartiles(features: pd.DataFrame) -> pd.Series:\n",
    "    pred = _load_pred_30d()\n",
    "    if pred is not None and \"patient_id\" in pred.columns:\n",
    "        score_col = None\n",
    "        for c in [\"yhat\",\"risk\",\"prob\",\"pred\",\"prediction\",\"p_event\"]:\n",
    "            if c in pred.columns:\n",
    "                score_col = c; break\n",
    "        if score_col is None and {\"mu0\",\"mu1\"}.issubset(set(pred.columns)):\n",
    "            # if only mu0/mu1 are present, use mu0 as risk under no treatment\n",
    "            score_col = \"mu0\"\n",
    "        if score_col is not None:\n",
    "            s = pred[[\"patient_id\", score_col]].dropna()\n",
    "            s[\"patient_id\"] = s[\"patient_id\"].astype(str)\n",
    "            m = features[[\"patient_id\"]].copy()\n",
    "            m[\"patient_id\"] = m[\"patient_id\"].astype(str)\n",
    "            m = m.merge(s, on=\"patient_id\", how=\"left\")\n",
    "            vals = m[score_col].fillna(m[score_col].median())\n",
    "            q = pd.qcut(vals, 4, labels=[\"Q1-Low\",\"Q2-Moderate\",\"Q3-High\",\"Q4-VeryHigh\"]) \n",
    "            return pd.Series(q.values, index=features.index, name=\"Risk_Quartile\")\n",
    "    # Fallback: use outcome_30d as a crude proxy (rare, but consistent)\n",
    "    col = f\"outcome_{HORIZON_DAYS}d\"\n",
    "    if col in features.columns:\n",
    "        vals = features[col].astype(float)\n",
    "        # push positives to top quartile when extremely sparse\n",
    "        if vals.max() <= 1:\n",
    "            q = pd.qcut(vals.rank(method=\"first\"), 4, labels=[\"Q1-Low\",\"Q2-Moderate\",\"Q3-High\",\"Q4-VeryHigh\"]) \n",
    "            return pd.Series(q.values, index=features.index, name=\"Risk_Quartile\")\n",
    "    return pd.Series([\"Q2-Moderate\"] * len(features), index=features.index, name=\"Risk_Quartile\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Build public dataset (engaged-only + collapsed notes)\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def build_public_df(n_cases=N_CASES, high_risk_ratio=HIGH_RISK_RATIO, seed=RANDOM_STATE) -> tuple[pd.DataFrame, pd.DataFrame | None]:\n",
    "    features = _load_features().copy()\n",
    "    interventions = _load_interventions().copy()\n",
    "    signal_risks = _load_signal_risks()\n",
    "\n",
    "    # Engaged-only subset (Block 1 should have created this flag)\n",
    "    if \"engaged_only_flag\" in features.columns:\n",
    "        features = features[features[\"engaged_only_flag\"] == 1].copy()\n",
    "        print(f\"Engaged-only features: n={len(features)}\")\n",
    "\n",
    "    # Attach DX_Summary\n",
    "    features[\"DX_Summary\"] = build_dx_summary(features)\n",
    "\n",
    "    # Build collapsed notes from interventions (only encounterOccurred == 'YES')\n",
    "    notes = pd.Series(dtype=str)\n",
    "    if not interventions.empty:\n",
    "        interventions = _normalize_interventions_columns(interventions)\n",
    "        if \"patient_id\" in interventions.columns:\n",
    "            df_notes = interventions.copy()\n",
    "            if \"encounterOccurred\" in df_notes.columns:\n",
    "                df_notes = df_notes[df_notes[\"encounterOccurred\"].astype(str).str.upper().eq(\"YES\")]\n",
    "            col = _pick_note_column(df_notes)\n",
    "            if col is not None:\n",
    "                tmp = (\n",
    "                    df_notes[[\"patient_id\", col]]\n",
    "                    .dropna()\n",
    "                    .assign(**{col: lambda d: d[col].astype(str).map(_clean_note_text)})\n",
    "                    .groupby(\"patient_id\")[col]\n",
    "                    .apply(lambda s: \"\\nâ€”\\n\".join([x for x in s if x.strip()]))\n",
    "                )\n",
    "                notes = tmp[tmp.astype(str).str.strip().ne(\"\")]\n",
    "            else:\n",
    "                print(\"âš ï¸ No note-like column in interventions; Encounter_Notes will be empty.\")\n",
    "        else:\n",
    "            print(\"âš ï¸ interventions has no 'patient_id'; cannot join notes.\")\n",
    "\n",
    "    features = features.merge(notes.rename(\"Encounter_Notes\"), left_on=\"patient_id\", right_index=True, how=\"left\")\n",
    "\n",
    "    # Signal risks summary per patient (optional)\n",
    "    signals_col = pd.Series([\"\" ] * len(features), index=features.index, name=\"Signals_Summary\")\n",
    "    if signal_risks is not None and {\"patient_id\",\"signal\"}.issubset(signal_risks.columns):\n",
    "        tmp = (\n",
    "            signal_risks[[\"patient_id\",\"signal\"]]\n",
    "            .dropna()\n",
    "            .assign(patient_id=lambda d: d[\"patient_id\"].astype(str))\n",
    "            .groupby(\"patient_id\")[\"signal\"].apply(lambda s: \"; \".join(sorted(set(map(str, s)))))\n",
    "        )\n",
    "        m = features[\"patient_id\"].astype(str)\n",
    "        signals_col = m.map(tmp).fillna(\"\")\n",
    "    features[\"Signals_Summary\"] = signals_col\n",
    "\n",
    "    # Risk quartiles for sampling & display\n",
    "    rq = label_risk_quartiles(features)\n",
    "    features[\"Risk_Quartile\"] = rq\n",
    "\n",
    "    # Keep only patients with at least some notes\n",
    "    base = features.copy()\n",
    "    base[\"has_notes\"] = base[\"Encounter_Notes\"].astype(str).str.strip().ne(\"\")\n",
    "    base = base[base[\"has_notes\"]]\n",
    "    if base.empty:\n",
    "        raise RuntimeError(\"No engaged patients with Encounter_Notes found after filtering.\")\n",
    "\n",
    "    # Sample: aim for HIGH_RISK_RATIO from Q4 and the remainder from Q1â€“Q3\n",
    "    rng = np.random.default_rng(seed)\n",
    "    q4 = base[base[\"Risk_Quartile\"].eq(\"Q4-VeryHigh\")]\n",
    "    q123 = base[~base.index.isin(q4.index)]\n",
    "\n",
    "    n_q4 = min(int(round(n_cases * high_risk_ratio)), len(q4))\n",
    "    n_q123 = min(n_cases - n_q4, len(q123))\n",
    "\n",
    "    sample_idx = list(rng.choice(q4.index, size=n_q4, replace=False))\n",
    "    needed = n_cases - len(sample_idx)\n",
    "    if needed > 0:\n",
    "        sample_idx += list(rng.choice(q123.index, size=needed, replace=False))\n",
    "\n",
    "    public = base.loc[sample_idx].copy().reset_index(drop=True)\n",
    "\n",
    "    # Case IDs\n",
    "    public.insert(0, \"Case_ID\", np.arange(1, len(public) + 1, dtype=int))\n",
    "\n",
    "    # Convenience demographics columns if available\n",
    "    for a,b in [(\"age\",\"Age\"),(\"gender\",\"Sex\"),(\"Race\",\"Race\")]:\n",
    "        if a in public.columns and b not in public.columns:\n",
    "            public.rename(columns={a:b}, inplace=True)\n",
    "\n",
    "    # Mapping file\n",
    "    mapping = public[[\"Case_ID\",\"patient_id\"]].copy()\n",
    "    mapping.to_csv(OUTPUT_DIR/\"clinical_validation_case_index_mapping.csv\", index=False)\n",
    "\n",
    "    return public, signal_risks\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Excel writer with dual reviewer columns & data validation\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def write_review_excel(public: pd.DataFrame, out_xlsx: Path, signal_risks_df: pd.DataFrame | None):\n",
    "    # Columns to keep/show first\n",
    "    base_cols = [c for c in [\n",
    "        \"Case_ID\",\"patient_id\",\"Age\",\"Sex\",\"Risk_Quartile\",\n",
    "        f\"outcome_{HORIZON_DAYS}d\" if f\"outcome_{HORIZON_DAYS}d\" in public.columns else None,\n",
    "        \"DX_Summary\",\"Signals_Summary\",\"Encounter_Notes\"\n",
    "    ] if c is not None and c in public.columns]\n",
    "\n",
    "    # Build reviewer columns (3 reviewers)\n",
    "    reviewers = [1,2,3]\n",
    "    risk_colnames = [f\"R{r}_Risk\" for r in reviewers]\n",
    "    act_colnames  = [f\"R{r}_Action\" for r in reviewers]\n",
    "    note_colnames = [f\"R{r}_Notes\" for r in reviewers]\n",
    "\n",
    "    review_df = public[base_cols].copy()\n",
    "    for c in risk_colnames + act_colnames + note_colnames:\n",
    "        review_df[c] = \"\"\n",
    "\n",
    "    # Write workbook (openpyxl engine)\n",
    "    with pd.ExcelWriter(out_xlsx, engine=\"openpyxl\") as writer:\n",
    "        review_df.to_excel(writer, index=False, sheet_name=\"review\")\n",
    "\n",
    "        # Helper sheet: codes (risks & interventions)\n",
    "        risks_list = [\"Q1-Low\",\"Q2-Moderate\",\"Q3-High\",\"Q4-VeryHigh\"]\n",
    "        interventions_list = [\n",
    "            \"watchful_waiting\",\"mental_health_support\",\"social_support\",\n",
    "            \"care_coordination\",\"pcp_followup\",\"specialist_referral\",\n",
    "            \"transportation_support\",\"medication_support\",\"housing_support\",\n",
    "            \"substance_use_support\",\"nutrition_support\",\"other\"\n",
    "        ]\n",
    "        # to avoid different lengths, pad to same length\n",
    "        L = max(len(risks_list), len(interventions_list))\n",
    "        def pad(lst, L): return lst + [\"\"] * (L - len(lst))\n",
    "        codes_df = pd.DataFrame({\n",
    "            \"Risk\": pad(risks_list, L),\n",
    "            \"Intervention\": pad(interventions_list, L)\n",
    "        })\n",
    "        codes_df.to_excel(writer, index=False, sheet_name=\"codes\")\n",
    "\n",
    "        ws = writer.sheets[\"review\"]\n",
    "        ws_codes = writer.sheets[\"codes\"]\n",
    "\n",
    "        # Freeze panes & basic widths\n",
    "        ws.freeze_panes = \"B2\"\n",
    "        widths = {\n",
    "            \"A\": 7,  # Case_ID\n",
    "            \"B\": 18, # patient_id\n",
    "            \"C\": 6,  # Age\n",
    "            \"D\": 9,  # Sex\n",
    "            \"E\": 13, # Risk_Quartile\n",
    "        }\n",
    "        # Find Encounter_Notes col index to set width + wrap\n",
    "        headers = [c.value for c in next(ws.iter_rows(min_row=1, max_row=1))]\n",
    "        col_idx = {h: i+1 for i,h in enumerate(headers)}\n",
    "        if \"Encounter_Notes\" in col_idx:\n",
    "            widths[get_column_letter(col_idx[\"Encounter_Notes\"])] = 65\n",
    "        if \"DX_Summary\" in col_idx:\n",
    "            widths[get_column_letter(col_idx[\"DX_Summary\"])] = 24\n",
    "        if \"Signals_Summary\" in col_idx:\n",
    "            widths[get_column_letter(col_idx[\"Signals_Summary\"])] = 28\n",
    "        for letter, w in widths.items():\n",
    "            ws.column_dimensions[letter].width = w\n",
    "\n",
    "        # Wrap text for notes columns\n",
    "        wrap_targets = [\"Encounter_Notes\"] + note_colnames\n",
    "        for name in wrap_targets:\n",
    "            if name in col_idx:\n",
    "                j = col_idx[name]\n",
    "                for cell in ws.iter_cols(min_col=j, max_col=j, min_row=2, max_row=ws.max_row):\n",
    "                    for c in cell:\n",
    "                        c.alignment = c.alignment.copy(wrap_text=True, vertical=\"top\")\n",
    "        # Align base text to top as well\n",
    "        for name in [\"DX_Summary\",\"Signals_Summary\"]:\n",
    "            if name in col_idx:\n",
    "                j = col_idx[name]\n",
    "                for cell in ws.iter_cols(min_col=j, max_col=j, min_row=2, max_row=ws.max_row):\n",
    "                    for c in cell:\n",
    "                        c.alignment = c.alignment.copy(wrap_text=True, vertical=\"top\")\n",
    "\n",
    "        # Data validation (drop-downs)\n",
    "        last_row = ws.max_row\n",
    "        # Named ranges on codes sheet\n",
    "        risk_range = f\"codes!$A$2:$A${1+len(risks_list)}\"\n",
    "        int_range  = f\"codes!$B$2:$B${1+len(interventions_list)}\"\n",
    "\n",
    "        dv_risk = DataValidation(type=\"list\", formula1=risk_range, allow_blank=True)\n",
    "        dv_act  = DataValidation(type=\"list\", formula1=int_range,  allow_blank=True)\n",
    "        ws.add_data_validation(dv_risk)\n",
    "        ws.add_data_validation(dv_act)\n",
    "        # Apply to each reviewer column\n",
    "        for cname in risk_colnames:\n",
    "            if cname in col_idx:\n",
    "                col_letter = get_column_letter(col_idx[cname])\n",
    "                dv_risk.add(f\"{col_letter}2:{col_letter}{last_row}\")\n",
    "        for cname in act_colnames:\n",
    "            if cname in col_idx:\n",
    "                col_letter = get_column_letter(col_idx[cname])\n",
    "                dv_act.add(f\"{col_letter}2:{col_letter}{last_row}\")\n",
    "\n",
    "        # README sheet\n",
    "        readme = pd.DataFrame({\"Instructions\": [\n",
    "            \"Each case is one engaged patient with collapsed encounter notes.\",\n",
    "            \"Fill BOTH columns per reviewer: Risk level and Next Best Action.\",\n",
    "            \"Use the drop-down lists for standardized coding.\",\n",
    "            \"Add any free-text rationale in your reviewer Notes column.\",\n",
    "            \"Risk_Quartile shown is model-based (30d) when available.\",\n",
    "        ]})\n",
    "        readme.to_excel(writer, index=False, sheet_name=\"README\")\n",
    "\n",
    "    print(f\"âœ“ Wrote Excel pack: {out_xlsx}\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# CSV mirror (handy for quick diffing or copy/paste workflows)\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def write_review_csv(public: pd.DataFrame, out_csv: Path):\n",
    "    # Keep the same order as Excel base columns\n",
    "    keep = [c for c in [\n",
    "        \"Case_ID\",\"patient_id\",\"Age\",\"Sex\",\"Risk_Quartile\",\n",
    "        f\"outcome_{HORIZON_DAYS}d\" if f\"outcome_{HORIZON_DAYS}d\" in public.columns else None,\n",
    "        \"DX_Summary\",\"Signals_Summary\",\"Encounter_Notes\"\n",
    "    ] if c is not None and c in public.columns]\n",
    "    out = public[keep].copy()\n",
    "    # Ensure newlines are kept inside a single CSV cell by quoting the field\n",
    "    out.to_csv(out_csv, index=False)\n",
    "    print(f\"âœ“ Wrote CSV mirror: {out_csv}\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Main\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def main():\n",
    "    public, signal_risks_df = build_public_df(n_cases=N_CASES, high_risk_ratio=HIGH_RISK_RATIO, seed=RANDOM_STATE)\n",
    "\n",
    "    # Write Excel + CSV\n",
    "    xlsx_path = OUTPUT_DIR/\"clinical_validation_200_cases.xlsx\"\n",
    "    csv_path  = OUTPUT_DIR/\"clinical_validation_200_cases.csv\"\n",
    "    write_review_excel(public, xlsx_path, signal_risks_df)\n",
    "    write_review_csv(public, csv_path)\n",
    "\n",
    "    # Print quick head\n",
    "    print(public.head(3).to_string(index=False))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594c7b59-67c0-4d83-9c2f-c620e909687a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "BLOCK 5 â€” CLINICAL VALIDATION ANALYSIS & MODEL COMPARISON (v2)\n",
    "==============================================================\n",
    "- Links reviewer CSV (Case_ID) back to patient_id via mapping from Block 4.\n",
    "- Loads best-model predictions for time-to-event (7/30/90/180d) and intervention selection, if available.\n",
    "- Compares:\n",
    "  â€¢ Interâ€‘rater agreement among clinicians (risk + intervention)\n",
    "    - Pairwise Cohen's Îº (supplement)\n",
    "    - Fleiss' Îº with bootstrap 95% CI (primary)\n",
    "  â€¢ Model vs each reviewer (risk + intervention)\n",
    "  â€¢ Reviewer/majority-vote vs outcome (sensitivity, specificity, PPV, NPV, F1)\n",
    "  â€¢ Model ROC/AUC vs 30d outcome with reviewer operating points overlaid (Highâ€‘only and Med+High)\n",
    "- Saves publication-ready tables/figures to /publication_outputs.\n",
    "\n",
    "Expected inputs (created by prior blocks):\n",
    "- /publication_outputs/clinical_validation_200_cases.csv\n",
    "- /publication_outputs/clinical_validation_case_index_mapping.csv  (Case_ID â†” patient_id)\n",
    "- /publication_outputs/best_tte_model_predictions.csv              (optional; per-patient probs)\n",
    "- /publication_outputs/best_intervention_model_predictions.csv     (optional; per-patient recs)\n",
    "- /publication_outputs/complete_results_*.json                     (optional; for best model names)\n",
    "\n",
    "Outputs:\n",
    "- table_clinval_risk_interrater.csv (pairwise Cohen)\n",
    "- table_clinval_risk_interrater_fleiss.csv (Fleiss' Îº + 95% CI)\n",
    "- table_clinval_intervention_interrater.csv (pairwise Cohen)\n",
    "- table_clinval_intervention_interrater_fleiss.csv (Fleiss' Îº + 95% CI)\n",
    "- table_clinval_risk_model_vs_reviewer.csv\n",
    "- table_clinval_risk_vs_outcome_thresholds.csv\n",
    "- table_clinval_intervention_agreement.csv\n",
    "- table_clinval_intervention_confusion_matrix.csv\n",
    "- figure_clinval_model_roc_30d.png (with reviewer operating points)\n",
    "- figure_clinval_intervention_confusion.png\n",
    "- clinical_validation_with_patient_and_model.csv (analysis-ready merged dataset)\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import json\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, roc_curve, confusion_matrix, cohen_kappa_score,\n",
    "    accuracy_score, precision_score, recall_score, f1_score\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "PUBDIR = Path(\"publication_outputs\")\n",
    "PUBDIR.mkdir(exist_ok=True)\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "###############################\n",
    "# I/O HELPERS\n",
    "###############################\n",
    "\n",
    "def _find_latest(glob_pat: str) -> Path | None:\n",
    "    files = sorted(PUBDIR.glob(glob_pat))\n",
    "    return files[-1] if files else None\n",
    "\n",
    "\n",
    "def load_inputs():\n",
    "    reviews_fp = PUBDIR / \"clinical_validation_200_cases.csv\"\n",
    "    mapping_fp = PUBDIR / \"clinical_validation_case_index_mapping.csv\"\n",
    "    if not reviews_fp.exists():\n",
    "        raise FileNotFoundError(f\"Missing {reviews_fp}. Run Block 4 to generate it.\")\n",
    "    if not mapping_fp.exists():\n",
    "        raise FileNotFoundError(f\"Missing {mapping_fp}. Run Block 4 to generate it.\")\n",
    "    reviews = pd.read_csv(reviews_fp)\n",
    "    mapping = pd.read_csv(mapping_fp)\n",
    "\n",
    "    # Try to load model prediction files (optional but recommended)\n",
    "    tte_pred_fp = _find_latest(\"best_tte_model_predictions*.csv\") or _find_latest(\"tte_predictions_*.csv\")\n",
    "    int_pred_fp = _find_latest(\"best_intervention_model_predictions*.csv\") or _find_latest(\"intervention_predictions_*.csv\")\n",
    "\n",
    "    tte_preds = pd.read_csv(tte_pred_fp) if tte_pred_fp else pd.DataFrame()\n",
    "    int_preds = pd.read_csv(int_pred_fp) if int_pred_fp else pd.DataFrame()\n",
    "\n",
    "    # Try to get best model names for annotation\n",
    "    results_fp = _find_latest(\"complete_results_*.json\")\n",
    "    best_tte_name = None\n",
    "    best_int_name = None\n",
    "    if results_fp and results_fp.exists():\n",
    "        try:\n",
    "            with open(results_fp) as f:\n",
    "                res = json.load(f)\n",
    "            # Pick best TTE by 30d AUC\n",
    "            best_tte_name = None\n",
    "            best_auc = -np.inf\n",
    "            for mname, mdict in res.get(\"time_to_event\", {}).items():\n",
    "                auc_30 = mdict.get(\"30d\", {}).get(\"auc\")\n",
    "                if auc_30 is not None and auc_30 > best_auc:\n",
    "                    best_auc = auc_30\n",
    "                    best_tte_name = mname\n",
    "            # Pick best intervention by kappa\n",
    "            best_int_name = None\n",
    "            best_k = -np.inf\n",
    "            for mname, md in res.get(\"intervention_selection\", {}).items():\n",
    "                k = md.get(\"kappa\")\n",
    "                if k is not None and k > best_k:\n",
    "                    best_k = k\n",
    "                    best_int_name = mname\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    return reviews, mapping, tte_preds, int_preds, best_tte_name, best_int_name\n",
    "\n",
    "###############################\n",
    "# NORMALIZATION\n",
    "###############################\n",
    "\n",
    "RISK_MAP_ORD = {\n",
    "    # maps to ordinal 0/1/2 (Low/Med/High)\n",
    "    \"low\": 0, \"l\": 0, \"lo\": 0,\n",
    "    \"medium\": 1, \"med\": 1, \"m\": 1,\n",
    "    \"high\": 2, \"hi\": 2, \"h\": 2,\n",
    "}\n",
    "\n",
    "INTERVENTION_NORMALIZE = {\n",
    "    # unify common strings to snake_case used by Block 1-4\n",
    "    \"substance use\": \"substance_use_support\",\n",
    "    \"substance_use\": \"substance_use_support\",\n",
    "    \"mental health\": \"mental_health_support\",\n",
    "    \"mental_health\": \"mental_health_support\",\n",
    "    \"chronic\": \"chronic_condition_management\",\n",
    "    \"housing\": \"housing_assistance\",\n",
    "    \"transport\": \"transportation_assistance\",\n",
    "    \"transportation\": \"transportation_assistance\",\n",
    "    \"food\": \"food_assistance\",\n",
    "    \"utility\": \"utility_assistance\",\n",
    "    \"childcare\": \"childcare_assistance\",\n",
    "    \"watchful waiting\": \"watchful_waiting\",\n",
    "    \"watchful_waiting\": \"watchful_waiting\",\n",
    "}\n",
    "\n",
    "\n",
    "def norm_risk_label(x: str | float | int) -> str:\n",
    "    if pd.isna(x):\n",
    "        return \"\"\n",
    "    s = str(x).strip().lower()\n",
    "    for key in (\"high\", \"medium\", \"low\", \"hi\", \"med\", \"lo\", \"h\", \"m\", \"l\"):\n",
    "        if key in s.split():\n",
    "            return {\"high\": \"High\", \"hi\": \"High\", \"h\": \"High\",\n",
    "                    \"medium\": \"Medium\", \"med\": \"Medium\", \"m\": \"Medium\",\n",
    "                    \"low\": \"Low\", \"lo\": \"Low\", \"l\": \"Low\"}[key]\n",
    "    return s.title()\n",
    "\n",
    "\n",
    "def risk_to_ord(s: str) -> int | None:\n",
    "    if not s:\n",
    "        return None\n",
    "    return {\"Low\": 0, \"Medium\": 1, \"High\": 2}.get(s, None)\n",
    "\n",
    "\n",
    "def risk_to_bin(s: str, threshold: str = \"High\") -> int | None:\n",
    "    ordv = risk_to_ord(s)\n",
    "    if ordv is None:\n",
    "        return None\n",
    "    if threshold == \"High\":\n",
    "        return 1 if ordv == 2 else 0\n",
    "    if threshold == \"MedPlus\":  # Medium or High considered positive\n",
    "        return 1 if ordv >= 1 else 0\n",
    "    return None\n",
    "\n",
    "\n",
    "def norm_intervention(x: str) -> str:\n",
    "    if pd.isna(x) or str(x).strip() == \"\":\n",
    "        return \"\"\n",
    "    s = str(x).strip().lower().replace(\"-\", \" \").replace(\"/\", \" \")\n",
    "    s = \" \".join(s.split())\n",
    "    if s in INTERVENTION_NORMALIZE:\n",
    "        return INTERVENTION_NORMALIZE[s]\n",
    "    for k, v in INTERVENTION_NORMALIZE.items():\n",
    "        if k in s:\n",
    "            return v\n",
    "    return s.replace(\" \", \"_\")\n",
    "\n",
    "###############################\n",
    "# METRICS, AGREEMENT & BOOTSTRAP\n",
    "###############################\n",
    "\n",
    "def bin_class_metrics(y_true, y_pred, y_score=None) -> dict:\n",
    "    y_true = np.asarray(y_true).astype(int)\n",
    "    y_pred = np.asarray(y_pred).astype(int)\n",
    "    out = {\n",
    "        \"n\": int(len(y_true)),\n",
    "        \"accuracy\": float(accuracy_score(y_true, y_pred)),\n",
    "        \"precision\": float(precision_score(y_true, y_pred, zero_division=0)),\n",
    "        \"recall\": float(recall_score(y_true, y_pred, zero_division=0)),\n",
    "        \"f1\": float(f1_score(y_true, y_pred, zero_division=0)),\n",
    "    }\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0,1]).ravel()\n",
    "    out.update({\n",
    "        \"tn\": int(tn), \"fp\": int(fp), \"fn\": int(fn), \"tp\": int(tp),\n",
    "        \"specificity\": float(tn / (tn + fp) if (tn + fp) else 0.0),\n",
    "        \"ppv\": float(tp / (tp + fp) if (tp + fp) else 0.0),\n",
    "        \"npv\": float(tn / (tn + fn) if (tn + fn) else 0.0),\n",
    "    })\n",
    "    if y_score is not None and len(np.unique(y_true)) == 2:\n",
    "        try:\n",
    "            out[\"auc\"] = float(roc_auc_score(y_true, y_score))\n",
    "        except Exception:\n",
    "            out[\"auc\"] = np.nan\n",
    "    else:\n",
    "        out[\"auc\"] = np.nan\n",
    "    return out\n",
    "\n",
    "\n",
    "def _fleiss_from_counts(counts: np.ndarray) -> tuple[float, dict]:\n",
    "    \"\"\"counts: (N_items x N_categories) integer ratings per item.\n",
    "    Returns (kappa, extras).\n",
    "    \"\"\"\n",
    "    counts = np.asarray(counts, dtype=float)\n",
    "    N, k = counts.shape\n",
    "    n_i = counts.sum(axis=1)  # raters per item (can vary)\n",
    "    # Remove items with <2 ratings\n",
    "    mask = n_i >= 2\n",
    "    counts = counts[mask]\n",
    "    n_i = n_i[mask]\n",
    "    if counts.shape[0] == 0:\n",
    "        return np.nan, {\"n_items\": 0}\n",
    "    # Item agreement\n",
    "    P_i = (np.sum(counts * (counts - 1), axis=1)) / (n_i * (n_i - 1))\n",
    "    P_bar = float(np.mean(P_i))\n",
    "    # Category proportions\n",
    "    p_j = np.sum(counts, axis=0) / np.sum(n_i)\n",
    "    P_e = float(np.sum(p_j ** 2))\n",
    "    kappa = (P_bar - P_e) / (1 - P_e) if (1 - P_e) > 0 else np.nan\n",
    "    extras = {\n",
    "        \"n_items\": int(counts.shape[0]),\n",
    "        \"mean_raters\": float(np.mean(n_i)),\n",
    "        \"P_bar\": P_bar, \"P_e\": P_e\n",
    "    }\n",
    "    return float(kappa), extras\n",
    "\n",
    "\n",
    "def _build_counts(df: pd.DataFrame, cols: list[str], categories: list[str]) -> np.ndarray:\n",
    "    cat_to_idx = {c: i for i, c in enumerate(categories)}\n",
    "    mat = []\n",
    "    for _, row in df[cols].iterrows():\n",
    "        votes = [str(v) for v in row.values if isinstance(v, str) and v.strip() != \"\"]\n",
    "        if len(votes) < 2:\n",
    "            continue\n",
    "        cnt = np.zeros(len(categories), dtype=int)\n",
    "        for v in votes:\n",
    "            if v in cat_to_idx:\n",
    "                cnt[cat_to_idx[v]] += 1\n",
    "        if cnt.sum() >= 2:\n",
    "            mat.append(cnt)\n",
    "    return np.array(mat, dtype=int)\n",
    "\n",
    "\n",
    "def fleiss_bootstrap(counts: np.ndarray, B: int = 1000, seed: int = RANDOM_STATE) -> tuple[float, float, float]:\n",
    "    \"\"\"Bootstrap 95% CI for Fleiss' kappa by resampling items with replacement.\"\"\"\n",
    "    counts = np.asarray(counts)\n",
    "    if counts.size == 0:\n",
    "        return np.nan, np.nan, np.nan\n",
    "    rng = np.random.default_rng(seed)\n",
    "    N = counts.shape[0]\n",
    "    ks = []\n",
    "    for _ in range(B):\n",
    "        idx = rng.integers(0, N, size=N)\n",
    "        k, _ = _fleiss_from_counts(counts[idx])\n",
    "        ks.append(k)\n",
    "    ks = np.array(ks)\n",
    "    return float(np.nanmean(ks)), float(np.nanpercentile(ks, 2.5)), float(np.nanpercentile(ks, 97.5))\n",
    "\n",
    "###############################\n",
    "# MAIN ANALYSIS\n",
    "###############################\n",
    "\n",
    "def main():\n",
    "    reviews, mapping, tte_preds, int_preds, best_tte_name, best_int_name = load_inputs()\n",
    "\n",
    "    # Merge Case_ID â†” patient_id\n",
    "    if \"Case_ID\" not in reviews.columns:\n",
    "        raise ValueError(\"Reviewer CSV missing Case_ID column.\")\n",
    "    if not {\"Case_ID\", \"patient_id\"}.issubset(mapping.columns):\n",
    "        raise ValueError(\"Mapping file must have Case_ID and patient_id.\")\n",
    "\n",
    "    df = reviews.merge(mapping[[\"Case_ID\", \"patient_id\"]], on=\"Case_ID\", how=\"left\")\n",
    "\n",
    "    # Normalize reviewer risk and intervention columns\n",
    "    risk_cols = [c for c in df.columns if c.startswith(\"Reviewer_\") and c.endswith(\"Risk_Assessment\")]\n",
    "    int_cols  = [c for c in df.columns if c.startswith(\"Reviewer_\") and c.endswith(\"Intervention\")]\n",
    "\n",
    "    for c in risk_cols:\n",
    "        df[c] = df[c].apply(norm_risk_label)\n",
    "    for c in int_cols:\n",
    "        df[c] = df[c].apply(norm_intervention)\n",
    "\n",
    "    # Outcome (expects 'Actual_30d_Outcome' as 'Event'/'No Event' from Block 4)\n",
    "    if \"Actual_30d_Outcome\" in df.columns:\n",
    "        df[\"y30\"] = df[\"Actual_30d_Outcome\"].map({\"Event\":1, \"No Event\":0})\n",
    "    else:\n",
    "        df[\"y30\"] = np.nan\n",
    "\n",
    "    # Attach model predictions if available\n",
    "    if not tte_preds.empty:\n",
    "        tte_cols = [c for c in tte_preds.columns if c.startswith(\"pred_\")]\n",
    "        keep_cols = [\"patient_id\"] + tte_cols\n",
    "        if \"model_name\" in tte_preds.columns:\n",
    "            keep_cols += [\"model_name\"]\n",
    "        df = df.merge(tte_preds[keep_cols].drop_duplicates(\"patient_id\"),\n",
    "                      on=\"patient_id\", how=\"left\")\n",
    "        if \"model_name\" in df.columns and best_tte_name is None:\n",
    "            best_tte_name = df[\"model_name\"].dropna().unique().tolist()\n",
    "    else:\n",
    "        print(\"[WARN] No best_tte_model_predictions*.csv found â€” AUC/ROC for model will be skipped.\")\n",
    "\n",
    "    if not int_preds.empty:\n",
    "        int_preds[\"recommended_intervention\"] = int_preds[\"recommended_intervention\"].apply(norm_intervention)\n",
    "        df = df.merge(int_preds.drop_duplicates(\"patient_id\"), on=\"patient_id\", how=\"left\")\n",
    "        if \"model_name\" in int_preds.columns and best_int_name is None:\n",
    "            best_int_name = int_preds[\"model_name\"].dropna().unique().tolist()\n",
    "    else:\n",
    "        print(\"[WARN] No best_intervention_model_predictions*.csv found â€” intervention agreement vs model will be limited.\")\n",
    "\n",
    "    # =========================\n",
    "    # 1) INTERâ€‘RATER AGREEMENT\n",
    "    # =========================\n",
    "    # Pairwise Cohen's Îº (supplement)\n",
    "    pair_rows = []\n",
    "    for i in range(len(risk_cols)):\n",
    "        for j in range(i+1, len(risk_cols)):\n",
    "            a, b = risk_cols[i], risk_cols[j]\n",
    "            sub = df[[a, b]].dropna()\n",
    "            if len(sub):\n",
    "                a_ord = sub[a].map({\"Low\":0, \"Medium\":1, \"High\":2})\n",
    "                b_ord = sub[b].map({\"Low\":0, \"Medium\":1, \"High\":2})\n",
    "                pair_rows.append({\"rater_a\": a, \"rater_b\": b, \"kappa\": cohen_kappa_score(a_ord, b_ord)})\n",
    "    kappa_risk_df = pd.DataFrame(pair_rows) if pair_rows else pd.DataFrame(columns=[\"rater_a\",\"rater_b\",\"kappa\"])\n",
    "    kappa_risk_df.to_csv(PUBDIR / \"table_clinval_risk_interrater.csv\", index=False)\n",
    "\n",
    "    pair_rows = []\n",
    "    for i in range(len(int_cols)):\n",
    "        for j in range(i+1, len(int_cols)):\n",
    "            a, b = int_cols[i], int_cols[j]\n",
    "            sub = df[[a, b]].replace(\"\", np.nan).dropna()\n",
    "            if len(sub):\n",
    "                pair_rows.append({\"rater_a\": a, \"rater_b\": b, \"kappa\": cohen_kappa_score(sub[a], sub[b])})\n",
    "    kappa_int_df = pd.DataFrame(pair_rows) if pair_rows else pd.DataFrame(columns=[\"rater_a\",\"rater_b\",\"kappa\"])\n",
    "    kappa_int_df.to_csv(PUBDIR / \"table_clinval_intervention_interrater.csv\", index=False)\n",
    "\n",
    "    # Fleiss' Îº (primary) + bootstrap CI\n",
    "    # Risk (Low/Med/High)\n",
    "    risk_categories = [\"Low\", \"Medium\", \"High\"]\n",
    "    risk_counts = _build_counts(df, risk_cols, risk_categories)\n",
    "    k_fleiss_risk, extra_risk = _fleiss_from_counts(risk_counts)\n",
    "    k_boot_mean, k_lo, k_hi = fleiss_bootstrap(risk_counts, B=1000, seed=RANDOM_STATE)\n",
    "    pd.DataFrame([{\"fleiss_kappa\": k_fleiss_risk, \"ci_mean\": k_boot_mean, \"ci_lo\": k_lo, \"ci_hi\": k_hi, **extra_risk}]).to_csv(\n",
    "        PUBDIR/\"table_clinval_risk_interrater_fleiss.csv\", index=False\n",
    "    )\n",
    "\n",
    "    # Intervention (multiâ€‘class)\n",
    "    # Build set of categories present (exclude blanks)\n",
    "    all_int_vals = set()\n",
    "    for c in int_cols:\n",
    "        all_int_vals.update([v for v in df[c].dropna().unique().tolist() if isinstance(v, str) and v.strip() != \"\"])\n",
    "    int_categories = sorted(all_int_vals)\n",
    "    if len(int_categories) >= 2:\n",
    "        int_counts = _build_counts(df, int_cols, int_categories)\n",
    "        k_fleiss_int, extra_int = _fleiss_from_counts(int_counts)\n",
    "        k_boot_mean_i, k_lo_i, k_hi_i = fleiss_bootstrap(int_counts, B=1000, seed=RANDOM_STATE)\n",
    "        pd.DataFrame([{\"fleiss_kappa\": k_fleiss_int, \"ci_mean\": k_boot_mean_i, \"ci_lo\": k_lo_i, \"ci_hi\": k_hi_i, **extra_int}]).to_csv(\n",
    "            PUBDIR/\"table_clinval_intervention_interrater_fleiss.csv\", index=False\n",
    "        )\n",
    "\n",
    "    # =========================\n",
    "    # 2) MODEL VS REVIEWER â€” RISK\n",
    "    # =========================\n",
    "    rows = []\n",
    "    if \"pred_30d\" in df.columns and df[\"y30\"].notna().any():\n",
    "        y_true = df[\"y30\"].dropna().astype(int)\n",
    "        y_score = df.loc[y_true.index, \"pred_30d\"].astype(float)\n",
    "        y_pred_m = (y_score >= 0.5).astype(int)\n",
    "        m_metrics = bin_class_metrics(y_true, y_pred_m, y_score)\n",
    "        m_metrics.update({\"who\": f\"Model ({best_tte_name or 'best_30d'})\", \"threshold\": 0.5})\n",
    "        rows.append(m_metrics)\n",
    "\n",
    "        # Reviewer operating points (High and Med+High)\n",
    "        op_points = []  # (fpr, tpr, label)\n",
    "        for thr_name in [\"High\", \"MedPlus\"]:\n",
    "            for r in risk_cols:\n",
    "                r_bin = df[r].apply(lambda s: risk_to_bin(s, thr_name))\n",
    "                mask = r_bin.notna() & df[\"y30\"].notna()\n",
    "                if mask.sum() == 0:\n",
    "                    continue\n",
    "                mets = bin_class_metrics(df.loc[mask, \"y30\"], r_bin[mask])\n",
    "                mets.update({\"who\": f\"{r} ({'High' if thr_name=='High' else 'Med+High'}=1)\", \"threshold\": thr_name})\n",
    "                rows.append(mets)\n",
    "                # For ROC overlay\n",
    "                tn, fp, fn, tp = mets[\"tn\"], mets[\"fp\"], mets[\"fn\"], mets[\"tp\"]\n",
    "                fpr = fp / (fp + tn) if (fp + tn) else 0.0\n",
    "                tpr = tp / (tp + fn) if (tp + fn) else 0.0\n",
    "                op_points.append((fpr, tpr, mets[\"who\"]))\n",
    "\n",
    "        # Majority vote thresholds\n",
    "        def maj_vote(series: pd.Series, thr: str) -> int | None:\n",
    "            bins = series.apply(lambda s: risk_to_bin(s, thr))\n",
    "            if bins.isna().all():\n",
    "                return None\n",
    "            ones = (bins == 1).sum()\n",
    "            zeros = (bins == 0).sum()\n",
    "            if ones == zeros:\n",
    "                return None\n",
    "            return 1 if ones > zeros else 0\n",
    "\n",
    "        for thr in [\"High\", \"MedPlus\"]:\n",
    "            mv = df[risk_cols].apply(lambda row: maj_vote(row, thr), axis=1)\n",
    "            mask = mv.notna() & df[\"y30\"].notna()\n",
    "            if mask.sum() > 0:\n",
    "                mets = bin_class_metrics(df.loc[mask, \"y30\"], mv[mask])\n",
    "                label = f\"Majority Vote ({'High' if thr=='High' else 'Med+High'}=1)\"\n",
    "                mets.update({\"who\": label, \"threshold\": thr})\n",
    "                rows.append(mets)\n",
    "                # ROC overlay point\n",
    "                tn, fp, fn, tp = mets[\"tn\"], mets[\"fp\"], mets[\"fn\"], mets[\"tp\"]\n",
    "                fpr = fp / (fp + tn) if (fp + tn) else 0.0\n",
    "                tpr = tp / (tp + fn) if (tp + fn) else 0.0\n",
    "                op_points.append((fpr, tpr, label))\n",
    "\n",
    "        # Plot ROC with reviewer operating points\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "        plt.figure(figsize=(7, 6))\n",
    "        plt.plot(fpr, tpr, lw=2, label=f\"Model ROC (AUC={m_metrics.get('auc', np.nan):.3f})\")\n",
    "        plt.plot([0,1],[0,1], linestyle=\"--\", alpha=0.4, label=\"Chance\")\n",
    "        # Overlay points\n",
    "        for fpr_p, tpr_p, lab in op_points:\n",
    "            plt.scatter([fpr_p], [tpr_p], s=60)\n",
    "            plt.annotate(lab, (fpr_p, tpr_p), textcoords=\"offset points\", xytext=(6,4), fontsize=8)\n",
    "        plt.xlabel(\"False Positive Rate\")\n",
    "        plt.ylabel(\"True Positive Rate\")\n",
    "        plt.title(\"Model ROC â€” 30d with Reviewer Operating Points\")\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(PUBDIR / \"figure_clinval_model_roc_30d.png\", dpi=300)\n",
    "        plt.close()\n",
    "    else:\n",
    "        print(\"[WARN] Missing model 30d predictions and/or outcomes; skipping model ROC and metrics vs outcome.\")\n",
    "\n",
    "    risk_perf_df = pd.DataFrame(rows)\n",
    "    risk_perf_df.to_csv(PUBDIR / \"table_clinval_risk_vs_outcome_thresholds.csv\", index=False)\n",
    "\n",
    "    # Kappa between model binary decision and each reviewer\n",
    "    rows = []\n",
    "    if \"pred_30d\" in df.columns:\n",
    "        m_dec = (df[\"pred_30d\"].astype(float) >= 0.5).astype(int)\n",
    "        for thr in [\"High\", \"MedPlus\"]:\n",
    "            for r in risk_cols:\n",
    "                r_bin = df[r].apply(lambda s: risk_to_bin(s, thr))\n",
    "                mask = r_bin.notna()\n",
    "                if mask.sum() == 0:\n",
    "                    continue\n",
    "                k = cohen_kappa_score(m_dec[mask], r_bin[mask])\n",
    "                rows.append({\"threshold\": thr, \"kappa\": k, \"reviewer\": r, \"model\": best_tte_name or \"best_30d\"})\n",
    "    risk_model_vs_reviewer_df = pd.DataFrame(rows)\n",
    "    risk_model_vs_reviewer_df.to_csv(PUBDIR / \"table_clinval_risk_model_vs_reviewer.csv\", index=False)\n",
    "\n",
    "    # ========================================\n",
    "    # 3) INTERVENTION â€” AGREEMENT & CONFUSION\n",
    "    # ========================================\n",
    "    inter_rows = []\n",
    "    if \"recommended_intervention\" in df.columns:\n",
    "        for r in int_cols:\n",
    "            sub = df[[r, \"recommended_intervention\"]].replace(\"\", np.nan).dropna()\n",
    "            if len(sub):\n",
    "                acc = accuracy_score(sub[r], sub[\"recommended_intervention\"])\n",
    "                f1m = f1_score(sub[r], sub[\"recommended_intervention\"], average=\"macro\")\n",
    "                kap = cohen_kappa_score(sub[r], sub[\"recommended_intervention\"])\n",
    "                inter_rows.append({\n",
    "                    \"reviewer\": r,\n",
    "                    \"model\": best_int_name or \"best_intervention\",\n",
    "                    \"accuracy\": acc,\n",
    "                    \"macro_f1\": f1m,\n",
    "                    \"kappa\": kap,\n",
    "                    \"n\": len(sub)\n",
    "                })\n",
    "        inter_df = pd.DataFrame(inter_rows)\n",
    "        inter_df.to_csv(PUBDIR / \"table_clinval_intervention_agreement.csv\", index=False)\n",
    "\n",
    "        # Majority reviewer vs model confusion\n",
    "        def maj_interv(row):\n",
    "            vals = [v for v in row if isinstance(v, str) and v]\n",
    "            if not vals:\n",
    "                return None\n",
    "            vc = pd.Series(vals).value_counts()\n",
    "            if len(vc) >= 2 and vc.iloc[0] == vc.iloc[1]:\n",
    "                return None\n",
    "            return vc.index[0]\n",
    "        df[\"reviewer_intervention_majority\"] = df[int_cols].apply(maj_interv, axis=1)\n",
    "        cm_df = df[[\"reviewer_intervention_majority\", \"recommended_intervention\"]].dropna()\n",
    "        if len(cm_df):\n",
    "            labels = sorted(pd.unique(pd.concat([cm_df.iloc[:,0], cm_df.iloc[:,1]])))\n",
    "            cm = confusion_matrix(cm_df.iloc[:,0], cm_df.iloc[:,1], labels=labels)\n",
    "            cm_tbl = pd.DataFrame(cm, index=[f\"R:{l}\" for l in labels], columns=[f\"M:{l}\" for l in labels])\n",
    "            cm_tbl.to_csv(PUBDIR / \"table_clinval_intervention_confusion_matrix.csv\")\n",
    "\n",
    "            plt.figure(figsize=(max(6, 0.5*len(labels)+2), max(5, 0.5*len(labels)+2)))\n",
    "            sns.heatmap(cm_tbl, annot=True, fmt=\"d\", cbar=False)\n",
    "            plt.title(\"Intervention â€” Majority Reviewer vs Model (counts)\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(PUBDIR / \"figure_clinval_intervention_confusion.png\", dpi=300)\n",
    "            plt.close()\n",
    "    else:\n",
    "        print(\"[WARN] No model recommended_intervention predictions loaded; skipping intervention agreement.\")\n",
    "\n",
    "    # =====================================\n",
    "    # 4) SAVE MERGED ANALYSIS DATASET\n",
    "    # =====================================\n",
    "    out_cols = [\"Case_ID\", \"patient_id\", \"y30\", \"pred_7d\", \"pred_30d\", \"pred_90d\", \"pred_180d\",\n",
    "                \"recommended_intervention\"] + risk_cols + int_cols\n",
    "    keep = [c for c in out_cols if c in df.columns]\n",
    "    df[keep].to_csv(PUBDIR / \"clinical_validation_with_patient_and_model.csv\", index=False)\n",
    "\n",
    "    print(\"âœ“ Clinical validation analysis complete. Outputs written to /publication_outputs.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a626816b-5681-4857-8fec-97d8fb61d57c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def plot_flow_diagram():\n",
    "    \"\"\"\n",
    "    Generates and saves the study flow diagram (Figure 1) using matplotlib.\n",
    "    This diagram shows the patient cohort selection and splitting process.\n",
    "    The output is saved to the 'publication_outputs' folder.\n",
    "    \"\"\"\n",
    "    # --- Define Output Path ---\n",
    "    output_dir = 'publication_outputs'\n",
    "    # Create the directory if it does not exist to prevent errors\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    output_path = os.path.join(output_dir, 'figure1_study_flow_diagram.png')\n",
    "\n",
    "    # --- Create a figure and a set of subplots ---\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "    # --- Define Box Styles ---\n",
    "    # Style for the main boxes (e.g., total assessed, included)\n",
    "    box_style_main = dict(boxstyle='round,pad=0.5', fc='lightblue', alpha=0.7)\n",
    "    # Style for the exclusion box\n",
    "    box_style_excluded = dict(boxstyle='round,pad=0.5', fc='lightcoral', alpha=0.7)\n",
    "    # Style for the final split boxes (train/test)\n",
    "    box_style_split = dict(boxstyle='round,pad=0.5', fc='lightyellow', alpha=0.7)\n",
    "\n",
    "    # --- Create Text Boxes with Sample Sizes ---\n",
    "    # Top box: Assessed for eligibility\n",
    "    ax.text(0.5, 0.9, 'Assessed for eligibility\\n(N = 157,411)', ha='center', va='center', bbox=box_style_main, fontsize=12)\n",
    "\n",
    "    # Exclusion box\n",
    "    ax.text(0.5, 0.7, 'Excluded (n = 1,780)\\n- Missing birthDate or gender', ha='center', va='center', bbox=box_style_excluded, fontsize=12)\n",
    "\n",
    "    # Included in analysis box\n",
    "    ax.text(0.5, 0.5, 'Included in analysis\\n(N = 155,631)', ha='center', va='center', bbox=box_style_main, fontsize=12)\n",
    "\n",
    "    # Final split boxes: Training and Test sets\n",
    "    ax.text(0.25, 0.3, 'Training Set\\n(n = 124,506)', ha='center', va='center', bbox=box_style_split, fontsize=12)\n",
    "    ax.text(0.75, 0.3, 'Test Set\\n(n = 31,125)', ha='center', va='center', bbox=box_style_split, fontsize=12)\n",
    "\n",
    "    # --- Draw Arrows ---\n",
    "    # Arrow from 'Assessed' to 'Excluded'\n",
    "    ax.arrow(0.5, 0.85, 0, -0.1, head_width=0.02, head_length=0.02, fc='black', ec='black')\n",
    "    \n",
    "    # Arrow from 'Excluded' to 'Included'\n",
    "    ax.arrow(0.5, 0.65, 0, -0.1, head_width=0.02, head_length=0.02, fc='black', ec='black')\n",
    "\n",
    "    # Arrows from 'Included' to the Train/Test split\n",
    "    ax.arrow(0.5, 0.45, -0.2, -0.1, head_width=0.02, head_length=0.02, fc='black', ec='black')\n",
    "    ax.arrow(0.5, 0.45, 0.2, -0.1, head_width=0.02, head_length=0.02, fc='black', ec='black')\n",
    "\n",
    "    # --- Final Plot Adjustments ---\n",
    "    # Remove the axis ticks and frame for a cleaner look\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_frame_on(False)\n",
    "    \n",
    "    # Set the limits of the plot to ensure everything fits\n",
    "    ax.set_ylim(0.2, 1)\n",
    "    ax.set_xlim(0, 1)\n",
    "\n",
    "    # Add a title to the plot\n",
    "    plt.title('Figure 1: Study Flow Diagram', fontsize=16)\n",
    "    \n",
    "    # --- Save the plot instead of displaying it ---\n",
    "    # Using bbox_inches='tight' removes excess whitespace around the figure\n",
    "    # Using dpi=300 provides a high resolution suitable for publication\n",
    "    plt.savefig(output_path, bbox_inches='tight', dpi=300)\n",
    "    \n",
    "    # Close the plot to free up memory\n",
    "    plt.close(fig)\n",
    "    \n",
    "    print(f\"Figure 1 successfully saved to: {output_path}\")\n",
    "\n",
    "# --- Run the function to generate and save the plot ---\n",
    "plot_flow_diagram()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e4a1e4-260c-44ee-86da-dfdc69062a3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}